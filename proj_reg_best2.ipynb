{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Osama\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "turkish_stopwords = stopwords.words('turkish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classification_df = pd.read_csv(\"/Users/Osama/Downloads/CS412PROJ/train-classification.csv\",)\n",
    "train_classification_df = train_classification_df.rename(columns={'Unnamed: 0': 'user_id', 'label': 'category'})\n",
    "\n",
    "# Unifying labels\n",
    "train_classification_df[\"category\"] = train_classification_df[\"category\"].apply(str.lower)\n",
    "username2_category = train_classification_df.set_index(\"user_id\").to_dict()[\"category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>art</th>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entertainment</th>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fashion</th>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>food</th>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gaming</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health and lifestyle</th>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mom and children</th>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sports</th>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tech</th>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>travel</th>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      user_id\n",
       "category                     \n",
       "art                       191\n",
       "entertainment             323\n",
       "fashion                   299\n",
       "food                      511\n",
       "gaming                     13\n",
       "health and lifestyle      503\n",
       "mom and children          149\n",
       "sports                    113\n",
       "tech                      346\n",
       "travel                    294"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_classification_df.groupby(\"category\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tech'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username2_category[\"kod8net\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_classification_df = pd.read_csv(\"/Users/Osama/Downloads/CS412PROJ/train-classification.csv\",)\\ntrain_classification_df = train_classification_df.rename(columns={\\'Unnamed: 0\\': \\'user_id\\', \\'label\\': \\'category\\'})\\n\\n# Unifying labels\\ntrain_classification_df[\"category\"] = train_classification_df[\"category\"].apply(str.lower)\\nusername2_category = train_classification_df.set_index(\"user_id\").to_dict()[\"category\"]\\n\\n\\n\\n\\n# Load the additional CSV file\\nadditional_data_path = \"/Users/Osama/Downloads/CS412PROJ/annotated_users_CS412-2753ef4cf74e.csv\"\\nadditional_data_df = pd.read_csv(additional_data_path)\\n\\n# Extract and rename the relevant columns\\nadditional_data_df = additional_data_df[[\\'Unnamed: 0\\', \\'influencerCategory\\']].rename(columns={\\n    \\'Unnamed: 0\\': \\'user_id\\', \\n    \\'influencerCategory\\': \\'category\\'\\n})\\n\\nadditional_data_df = additional_data_df.dropna()\\n\\n\\n\\n# Convert the \\'category\\' column to string type and apply .lower(), handling any NaN or unexpected values\\nadditional_data_df[\\'category\\'] = additional_data_df[\\'category\\'].astype(str).fillna(\\'\\').apply(str.lower)\\n\\n# Append the new data to the original train_classification_df\\ntrain_classification_df = pd.concat([train_classification_df, additional_data_df], ignore_index=True)\\n\\n# Update the username2_category dictionary with the new data\\nusername2_category.update(additional_data_df.set_index(\"user_id\").to_dict()[\"category\"])\\n\\n# Check the updated data by viewing the first few rows\\nprint(train_classification_df.head())\\n\\n# Re-check the category distribution\\ntrain_classification_df.groupby(\"category\").count()'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''train_classification_df = pd.read_csv(\"/Users/Osama/Downloads/CS412PROJ/train-classification.csv\",)\n",
    "train_classification_df = train_classification_df.rename(columns={'Unnamed: 0': 'user_id', 'label': 'category'})\n",
    "\n",
    "# Unifying labels\n",
    "train_classification_df[\"category\"] = train_classification_df[\"category\"].apply(str.lower)\n",
    "username2_category = train_classification_df.set_index(\"user_id\").to_dict()[\"category\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the additional CSV file\n",
    "additional_data_path = \"/Users/Osama/Downloads/CS412PROJ/annotated_users_CS412-2753ef4cf74e.csv\"\n",
    "additional_data_df = pd.read_csv(additional_data_path)\n",
    "\n",
    "# Extract and rename the relevant columns\n",
    "additional_data_df = additional_data_df[['Unnamed: 0', 'influencerCategory']].rename(columns={\n",
    "    'Unnamed: 0': 'user_id', \n",
    "    'influencerCategory': 'category'\n",
    "})\n",
    "\n",
    "additional_data_df = additional_data_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "# Convert the 'category' column to string type and apply .lower(), handling any NaN or unexpected values\n",
    "additional_data_df['category'] = additional_data_df['category'].astype(str).fillna('').apply(str.lower)\n",
    "\n",
    "# Append the new data to the original train_classification_df\n",
    "train_classification_df = pd.concat([train_classification_df, additional_data_df], ignore_index=True)\n",
    "\n",
    "# Update the username2_category dictionary with the new data\n",
    "username2_category.update(additional_data_df.set_index(\"user_id\").to_dict()[\"category\"])\n",
    "\n",
    "# Check the updated data by viewing the first few rows\n",
    "print(train_classification_df.head())\n",
    "\n",
    "# Re-check the category distribution\n",
    "train_classification_df.groupby(\"category\").count()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"/Users/Osama/Downloads/CS412PROJ/training-dataset.jsonl.gz\"\n",
    "\n",
    "username2posts_train = dict()\n",
    "username2profile_train = dict()\n",
    "\n",
    "username2posts_test = dict()\n",
    "username2profile_test = dict()\n",
    "\n",
    "\n",
    "with gzip.open(train_data_path, \"rt\") as fh:\n",
    "  for line in fh:\n",
    "    sample = json.loads(line)\n",
    "\n",
    "    profile = sample[\"profile\"]\n",
    "    username = profile[\"username\"]\n",
    "    if username in username2_category:\n",
    "      # train data info\n",
    "      username2posts_train[username] = sample[\"posts\"]\n",
    "      username2profile_train[username] = profile\n",
    "\n",
    "\n",
    "    else:\n",
    "      # it is test data info\n",
    "      username2posts_test[username] = sample[\"posts\"]\n",
    "      username2profile_test[username] = profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2741, 44)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_profile_df = pd.DataFrame(username2profile_train).T.reset_index(drop=True)\n",
    "test_profile_df = pd.DataFrame(username2profile_test).T.reset_index(drop=True)\n",
    "\n",
    "train_profile_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>id</th>\n",
       "      <th>full_name</th>\n",
       "      <th>biography</th>\n",
       "      <th>category_name</th>\n",
       "      <th>post_count</th>\n",
       "      <th>follower_count</th>\n",
       "      <th>following_count</th>\n",
       "      <th>is_business_account</th>\n",
       "      <th>is_private</th>\n",
       "      <th>...</th>\n",
       "      <th>business_category_name</th>\n",
       "      <th>overall_category_name</th>\n",
       "      <th>category_enum</th>\n",
       "      <th>is_verified_by_mv4b</th>\n",
       "      <th>is_regulated_c18</th>\n",
       "      <th>profile_pic_url</th>\n",
       "      <th>should_show_category</th>\n",
       "      <th>should_show_public_contacts</th>\n",
       "      <th>show_account_transparency_details</th>\n",
       "      <th>profile_picture_base64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beyazyakaliyiz</td>\n",
       "      <td>8634457436</td>\n",
       "      <td>Selam Beyaz Yakalı</td>\n",
       "      <td>Beyaz yakalıların dünyasına hoşgeldiniz 😀😀😀</td>\n",
       "      <td>Personal blog</td>\n",
       "      <td>None</td>\n",
       "      <td>1265</td>\n",
       "      <td>665</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>PERSONAL_BLOG</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://instagram.fist6-1.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>totalenergies_istasyonlari</td>\n",
       "      <td>7066643793</td>\n",
       "      <td>TotalEnergies İstasyonları</td>\n",
       "      <td>TotalEnergies İstasyonları resmi Instagram hes...</td>\n",
       "      <td>Energy Company</td>\n",
       "      <td>None</td>\n",
       "      <td>28025</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ENERGY_COMPANY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://instagram.fsaw2-1.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     username          id                    full_name  \\\n",
       "0              beyazyakaliyiz  8634457436           Selam Beyaz Yakalı   \n",
       "1  totalenergies_istasyonlari  7066643793  TotalEnergies İstasyonları   \n",
       "\n",
       "                                           biography   category_name  \\\n",
       "0        Beyaz yakalıların dünyasına hoşgeldiniz 😀😀😀   Personal blog   \n",
       "1  TotalEnergies İstasyonları resmi Instagram hes...  Energy Company   \n",
       "\n",
       "  post_count follower_count following_count is_business_account is_private  \\\n",
       "0       None           1265             665                True      False   \n",
       "1       None          28025               4                True      False   \n",
       "\n",
       "   ... business_category_name overall_category_name   category_enum  \\\n",
       "0  ...                   None                  None   PERSONAL_BLOG   \n",
       "1  ...                   None                  None  ENERGY_COMPANY   \n",
       "\n",
       "  is_verified_by_mv4b is_regulated_c18  \\\n",
       "0               False            False   \n",
       "1               False            False   \n",
       "\n",
       "                                     profile_pic_url should_show_category  \\\n",
       "0  https://instagram.fist6-1.fna.fbcdn.net/v/t51....                 True   \n",
       "1  https://instagram.fsaw2-1.fna.fbcdn.net/v/t51....                 True   \n",
       "\n",
       "  should_show_public_contacts show_account_transparency_details  \\\n",
       "0                        True                              True   \n",
       "1                        True                              True   \n",
       "\n",
       "                              profile_picture_base64  \n",
       "0  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
       "1  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
       "\n",
       "[2 rows x 44 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_profile_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bir            192.474290\n",
      "the            106.220712\n",
      "and             65.960927\n",
      "to              59.345598\n",
      "of              59.225437\n",
      "olsun           56.723444\n",
      "yeni            46.531255\n",
      "istanbul        46.083638\n",
      "in              44.908546\n",
      "olarak          43.520030\n",
      "olan            41.210234\n",
      "with            39.918309\n",
      "kutlu           39.892111\n",
      "kutlu olsun     39.315072\n",
      "iyi             37.609096\n",
      "güzel           37.207437\n",
      "kadar           36.265992\n",
      "you             35.986349\n",
      "for             34.862311\n",
      "özel            32.566009\n",
      "bilgi           32.489645\n",
      "devam           32.122644\n",
      "var             31.813758\n",
      "our             31.544601\n",
      "kasım           31.361084\n",
      "günü            31.188695\n",
      "birlikte        30.770312\n",
      "büyük           30.195825\n",
      "ilk             29.995598\n",
      "teşekkür        29.513940\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "\n",
    "def preprocess_text(text: str):\n",
    "    \n",
    "\n",
    "    # lower casing Turkish Text, Don't use str.lower :)\n",
    "    text = text.casefold()\n",
    "\n",
    "    #text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove special characters and punctuation\n",
    "    # HERE THE EMOJIS stuff are being removed, you may want to keep them :D\n",
    "    text = re.sub(r'[^a-zçğıöşü0-9\\s#@]', '', text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "corpus = []\n",
    "\n",
    "# to keep the label order\n",
    "train_usernames = []\n",
    "\n",
    "for username, posts in username2posts_train.items():\n",
    "  train_usernames.append(username)\n",
    "\n",
    "  # aggregating the posts per user\n",
    "  cleaned_captions = []\n",
    "  for post in posts:\n",
    "    post_caption = post.get(\"caption\", \"\")\n",
    "    if post_caption is None:\n",
    "      continue\n",
    "\n",
    "    post_caption = preprocess_text(post_caption)\n",
    "\n",
    "    if post_caption != \"\":\n",
    "      cleaned_captions.append(post_caption)\n",
    "\n",
    "\n",
    "  # joining the posts of each user with a \\n\n",
    "  user_post_captions = \"\\n\".join(cleaned_captions)\n",
    "  corpus.append(user_post_captions)\n",
    "\n",
    "\n",
    "\n",
    "#custom_stopwords = list(set(turkish_stopwords).union({\n",
    " #   'the', 'and', 'with', 'for', 'you', 'to', 'of', 'in', 'our', 'your', 'is', 'are','bir'\n",
    "#}))\n",
    "vectorizer = TfidfVectorizer(stop_words=turkish_stopwords, max_features=10000,min_df=10,ngram_range=(1, 3))\n",
    "\n",
    "# fit the vectorizer\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "# transform the data into vectors\n",
    "x_post_train = vectorizer.transform(corpus)\n",
    "y_train = [username2_category.get(uname, \"NA\") for uname in train_usernames]\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Inspect the frequency of each word\n",
    "df_tfidf = pd.DataFrame(x_post_train.toarray(), columns=feature_names)\n",
    "\n",
    "# Show the most frequent words (words in many posts)\n",
    "print(df_tfidf.sum().sort_values(ascending=False).head(30))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_usernames = []\n",
    "test_corpus = []\n",
    "for username, posts in username2posts_test.items():\n",
    "  test_usernames.append(username)\n",
    "  # aggregating the posts per user\n",
    "  cleaned_captions = []\n",
    "  for post in posts:\n",
    "    post_caption = post.get(\"caption\", \"\")\n",
    "    if post_caption is None:\n",
    "      continue\n",
    "\n",
    "    post_caption = preprocess_text(post_caption)\n",
    "\n",
    "    if post_caption != \"\":\n",
    "      cleaned_captions.append(post_caption)\n",
    "\n",
    "  user_post_captions = \"\\n\".join(cleaned_captions)\n",
    "  test_corpus.append(user_post_captions)\n",
    "\n",
    "\n",
    "# Just transforming! No Fitting!!!!!\n",
    "x_post_test = vectorizer.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure everything is fine\n",
    "assert y_train.count(\"NA\") == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ab', 'abant', 'abd', ..., 'şıklık', 'şıklığı', 'şımartın'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>abant</th>\n",
       "      <th>abd</th>\n",
       "      <th>abdulkadir</th>\n",
       "      <th>abdullah</th>\n",
       "      <th>abi</th>\n",
       "      <th>abiye</th>\n",
       "      <th>abone</th>\n",
       "      <th>about</th>\n",
       "      <th>about the</th>\n",
       "      <th>...</th>\n",
       "      <th>şöyle</th>\n",
       "      <th>şükran</th>\n",
       "      <th>şükranla</th>\n",
       "      <th>şükranlarımızı</th>\n",
       "      <th>şükür</th>\n",
       "      <th>şık</th>\n",
       "      <th>şık bir</th>\n",
       "      <th>şıklık</th>\n",
       "      <th>şıklığı</th>\n",
       "      <th>şımartın</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ab  abant  abd  abdulkadir  abdullah  abi  abiye  abone  about  about the  \\\n",
       "0  0.0    0.0  0.0         0.0       0.0  0.0    0.0    0.0    0.0        0.0   \n",
       "1  0.0    0.0  0.0         0.0       0.0  0.0    0.0    0.0    0.0        0.0   \n",
       "\n",
       "   ...  şöyle  şükran  şükranla  şükranlarımızı  şükür       şık  şık bir  \\\n",
       "0  ...    0.0     0.0       0.0             0.0    0.0  0.042156      0.0   \n",
       "1  ...    0.0     0.0       0.0             0.0    0.0  0.000000      0.0   \n",
       "\n",
       "   şıklık  şıklığı  şımartın  \n",
       "0     0.0      0.0       0.0  \n",
       "1     0.0      0.0       0.0  \n",
       "\n",
       "[2 rows x 10000 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf = pd.DataFrame(x_post_train.toarray(), columns=feature_names)\n",
    "df_tfidf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2741, 10000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df_tfidf: (2741, 10000)\n",
      "Length of y_train: 2741\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of df_tfidf:\", df_tfidf.shape)\n",
    "print(\"Length of y_train:\", len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(df_tfidf, y_train, test_size=0.2, stratify=y_train,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2192, 10000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(549, 10000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ozhotelstr\n",
      "elleturkiye\n",
      "sozerinsaatorhangazi\n",
      "sanliurfapiazzaavym\n",
      "rusanozden\n",
      "*****\n",
      "['ozhotelstr', 'elleturkiye', 'sozerinsaatorhangazi', 'sanliurfapiazzaavym', 'rusanozden']\n"
     ]
    }
   ],
   "source": [
    "#@title Test Data\n",
    "test_data_path = \"/Users/Osama/Downloads/CS412PROJ/test-classification-round1.dat\"\n",
    "\n",
    "with open(test_data_path, \"rt\") as fh:\n",
    "    for i, line in enumerate(fh):\n",
    "        print(line.strip())\n",
    "        if i >= 4:  # Stop after 5 lines\n",
    "            break\n",
    "\n",
    "print(\"*****\")\n",
    "\n",
    "test_unames = []\n",
    "with open(test_data_path, \"rt\") as fh:\n",
    "  for line in fh:\n",
    "    test_unames.append(line.strip())\n",
    "\n",
    "print(test_unames[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "screenname\n"
     ]
    }
   ],
   "source": [
    "x_test = []\n",
    "\n",
    "for uname in test_unames:\n",
    "  try:\n",
    "    index = test_usernames.index(uname)\n",
    "    x_test.append(x_post_test[index].toarray()[0])\n",
    "  except Exception as e:\n",
    "    try:\n",
    "      index = train_usernames.index(uname)\n",
    "      x_test.append(x_post_train[index].toarray()[0])\n",
    "    except Exception as e:\n",
    "      print(uname)\n",
    "\n",
    "\n",
    "test_unames.remove(\"screenname\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>abant</th>\n",
       "      <th>abd</th>\n",
       "      <th>abdulkadir</th>\n",
       "      <th>abdullah</th>\n",
       "      <th>abi</th>\n",
       "      <th>abiye</th>\n",
       "      <th>abone</th>\n",
       "      <th>about</th>\n",
       "      <th>about the</th>\n",
       "      <th>...</th>\n",
       "      <th>şöyle</th>\n",
       "      <th>şükran</th>\n",
       "      <th>şükranla</th>\n",
       "      <th>şükranlarımızı</th>\n",
       "      <th>şükür</th>\n",
       "      <th>şık</th>\n",
       "      <th>şık bir</th>\n",
       "      <th>şıklık</th>\n",
       "      <th>şıklığı</th>\n",
       "      <th>şımartın</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007865</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024666</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010521</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ab  abant       abd  abdulkadir  abdullah  abi  abiye  abone  about  \\\n",
       "0  0.0    0.0  0.000000         0.0       0.0  0.0    0.0    0.0    0.0   \n",
       "1  0.0    0.0  0.007865         0.0       0.0  0.0    0.0    0.0    0.0   \n",
       "\n",
       "   about the  ...     şöyle  şükran  şükranla  şükranlarımızı  şükür  \\\n",
       "0        0.0  ...  0.000000     0.0       0.0             0.0    0.0   \n",
       "1        0.0  ...  0.024666     0.0       0.0             0.0    0.0   \n",
       "\n",
       "        şık  şık bir  şıklık  şıklığı  şımartın  \n",
       "0  0.000000      0.0     0.0      0.0       0.0  \n",
       "1  0.010521      0.0     0.0      0.0       0.0  \n",
       "\n",
       "[2 rows x 10000 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame(np.array(x_test), columns=feature_names)\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_like_count(username, current_post=None):\n",
    "  def get_avg_like_count(posts:list):\n",
    "    total = 0.\n",
    "    for post in posts:\n",
    "      if current_post is not None and post[\"id\"] == current_post[\"id\"]:\n",
    "        continue\n",
    "\n",
    "      like_count = post.get(\"like_count\", 0)\n",
    "      if like_count is None:\n",
    "        like_count = 0\n",
    "      total += like_count\n",
    "\n",
    "    if len(posts) == 0:\n",
    "      return 0.\n",
    "\n",
    "    return total / len(posts)\n",
    "\n",
    "  if username in username2posts_train:\n",
    "    return get_avg_like_count(username2posts_train[username])\n",
    "  elif username in username2posts_test:\n",
    "    return get_avg_like_count(username2posts_test[username])\n",
    "  else:\n",
    "    print(f\"No data available for {username}\")\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_mse_like_counts(y_true, y_pred):\n",
    "  \"\"\"\n",
    "  Calculate the Log Mean Squared Error (Log MSE) for like counts (log(like_count + 1)).\n",
    "\n",
    "  Parameters:\n",
    "  - y_true: array-like, actual like counts\n",
    "  - y_pred: array-like, predicted like counts\n",
    "\n",
    "  Returns:\n",
    "  - log_mse: float, Log Mean Squared Error\n",
    "  \"\"\"\n",
    "  # Ensure inputs are numpy arrays\n",
    "  y_true = np.array(y_true)\n",
    "  y_pred = np.array(y_pred)\n",
    "\n",
    "  # Log transformation: log(like_count + 1)\n",
    "  log_y_true = np.log1p(y_true)\n",
    "  log_y_pred = np.log1p(y_pred)\n",
    "\n",
    "  # Compute squared errors\n",
    "  squared_errors = (log_y_true - log_y_pred) ** 2\n",
    "\n",
    "  # Return the mean of squared errors\n",
    "  return np.mean(squared_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log MSE Train= 1.2271047744059362\n"
     ]
    }
   ],
   "source": [
    "#@title Train Dataset evaluation\n",
    "\n",
    "y_like_count_train_true = []\n",
    "y_like_count_train_pred = []\n",
    "for uname, posts in username2posts_train.items():\n",
    "  for post in posts:\n",
    "    pred_val = predict_like_count(uname, post)\n",
    "    true_val = post.get(\"like_count\", 0)\n",
    "    if true_val is None:\n",
    "      true_val = 0\n",
    "\n",
    "    y_like_count_train_true.append(true_val)\n",
    "    y_like_count_train_pred.append(pred_val)\n",
    "\n",
    "print(f\"Log MSE Train= {log_mse_like_counts(y_like_count_train_true, y_like_count_train_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Test Dataset\n",
    "\n",
    "path = \"/Users/Osama/Downloads/CS412PROJ/test-regression-round1.jsonl\"\n",
    "output_path = \"/Users/Osama/Downloads/CS412PROJ/testy-regression-round1.jsonl\"\n",
    "\n",
    "to_predict_like_counts_usernames = []\n",
    "output_list = []\n",
    "with open(path, \"rt\") as fh:\n",
    "  for line in fh:\n",
    "    sample = json.loads(line)\n",
    "\n",
    "    # let's predict\n",
    "    pred_val = predict_like_count(sample[\"username\"])\n",
    "    sample[\"like_count\"] = int(pred_val)\n",
    "    output_list.append(sample)\n",
    "\n",
    "with open(output_path, \"wt\") as of:\n",
    "  json.dump(output_list, of)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'caption': 'KOZA 2023 2.si Damla’nın koleksiyonu, Latincede ‘Memento Mori’ '\n",
      "             'olarak bilinen ‘ölümlü olduğunu hatırla’ anlamındaki ifadeden '\n",
      "             'esinleniyor. Koleksiyon, hayatın ve ölümün, para, işçi, kral ve '\n",
      "             'kraliçe kavramları üzerinden yaratıcı görünümlerle bir araya '\n",
      "             'getirilmesini amaçlıyor. Ölüm sembollerinden esinlenen desenler '\n",
      "             'kullanan Damla, “kağıt parçasından ibaret olmak” kavramını '\n",
      "             'vurguluyor. Koleksiyon, yaşamın ve ölümün aynı anda ifade '\n",
      "             'edilmesini hedefliyor; kırmızı ve mavi ışıklarla veya '\n",
      "             'gözlüklerle görülen hologram efekti kullanılarak bu konsept '\n",
      "             'sahneye taşınıyor. Kırmızı renk ölümü, mavi ise yaşamı '\n",
      "             'simgeliyor. Koleksiyon, ofis giyimlerinden esinlenerek '\n",
      "             'kravatlar, gömlekler ve evrak çantaları içeriyor. Klasik sivri '\n",
      "             'burun çizmelerin üzerine spor ayakkabıların üst yüzeyi '\n",
      "             'yerleştirilerek, iş dünyasının koşuşturması ve cenaze '\n",
      "             'temalarının aynı anda ifade edilmesi amaçlanıyor. Para kazanma '\n",
      "             'arzusu, kırmızı zambak desenleri ve büyük mücevher görünümleri '\n",
      "             'ile koleksiyon tamamlanıyor.\\n'\n",
      "             '\\n'\n",
      "             'Tebrikler Damla!\\n'\n",
      "             '\\n'\n",
      "             '#GencModaTasarimcilari #Koza2023 #KozaYarismasi '\n",
      "             '#TasarimYarismasi #Moda #Fashion #ModaTasarımı',\n",
      "  'comments_count': 2,\n",
      "  'id': '18144550534306740',\n",
      "  'like_count': 158,\n",
      "  'media_type': 'CAROUSEL_ALBUM',\n",
      "  'media_url': 'https://scontent-sof1-1.cdninstagram.com/v/t51.29350-15/397997154_1016992459537522_4925783512176260397_n.jpg?_nc_cat=106&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=7V_eObkFeK4AX-LMtsK&_nc_ht=scontent-sof1-1.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfDEqDhzaTO3ezV-veT6cJFCOcAEyeVzHR6si9n33N6G5A&oe=6551B6B9',\n",
      "  'timestamp': '2023-11-02 15:49:22',\n",
      "  'username': 'kozayarismasi'},\n",
      " {'caption': 'Tüm Türkiye ve Avrupa’ya sevkiyatlarımız aralıksız devam ediyor! '\n",
      "             'Aracımız Bursa’dan Ordu’ya müşterimizin ürünleri için yola '\n",
      "             'çıkıyor.\\n'\n",
      "             '\\n'\n",
      "             '👉Tuna Mah. Etibank Cad. No:134 Osmangazi/BURSA\\n'\n",
      "             '\\n'\n",
      "             'www.celikbeymobilya.com sitemizden tüm modelleri '\n",
      "             'inceleyebilirsiniz. \\n'\n",
      "             '\\n'\n",
      "             '#bursa #almanya #fransa',\n",
      "  'comments_count': 0,\n",
      "  'id': '17995331788956693',\n",
      "  'like_count': 99,\n",
      "  'media_type': 'VIDEO',\n",
      "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m82/BF4767CB85BDFB8ADCCCA8F15B8C20B5_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmNsaXBzLnVua25vd24tQzMuNzIwLmRhc2hfYmFzZWxpbmVfMV92MSJ9&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=110&vs=1259525061418244_1441854817&_nc_vs=HBksFQIYT2lnX3hwdl9yZWVsc19wZXJtYW5lbnRfcHJvZC9CRjQ3NjdDQjg1QkRGQjhBRENDQ0E4RjE1QjhDMjBCNV92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dBRWdfaFZfcDVCYk5HZ0NBQTlzVURvZW5mZ3FicV9FQUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJvS3uOiQ0P8%2FFQIoAkMzLBdAJO%2Bdsi0OVhgSZGFzaF9iYXNlbGluZV8xX3YxEQB1AAA%3D&ccb=9-4&oh=00_AfAm22JssMPaUlQe3rpYsFWBhFb5mUgolTCdhV0Xgm4AnA&oe=6556A482&_nc_sid=1d576d&_nc_rid=cd9a998e44',\n",
      "  'timestamp': '2023-08-19 13:46:02',\n",
      "  'username': 'celikbeymobilya'},\n",
      " {'caption': '🤩\\n'\n",
      "             '\\n'\n",
      "             '.\\n'\n",
      "             '.\\n'\n",
      "             'Daha Fazlası İçin Beğenmeyi ve Takip Etmeyi Unutmayın\\n'\n",
      "             '.\\n'\n",
      "             'girisimci_muhendis ✅\\n'\n",
      "             '.\\n'\n",
      "             '📣Bu Bilgi Hakkında Ne Düşünüyorsunuz.\\n'\n",
      "             '.\\n'\n",
      "             '✅Görmesini İstediğin Arkadaşını Etiketle.\\n'\n",
      "             '.\\n'\n",
      "             '🔔Gönderi Bildirimlerini Açarak, Bilgileri Anında '\n",
      "             'Öğrenebilirsiniz.\\n'\n",
      "             '\\n'\n",
      "             '.\\n'\n",
      "             '\\n'\n",
      "             'Source: Unknown\\n'\n",
      "             'Dm for Credit or removal\\n'\n",
      "             '.\\n'\n",
      "             '.............................................................\\n'\n",
      "             'All rights and credits reserved to the respective owner(s). If '\n",
      "             'you are the main copyright owner rather than the one mentioned '\n",
      "             'here of this content, contact me to claim credit or content '\n",
      "             'removal',\n",
      "  'comments_count': 75,\n",
      "  'id': '18302703232191518',\n",
      "  'like_count': 1224,\n",
      "  'media_type': 'VIDEO',\n",
      "  'media_url': None,\n",
      "  'timestamp': '2023-10-02 06:53:33',\n",
      "  'username': 'girisimci_muhendis'}]\n"
     ]
    }
   ],
   "source": [
    "pprint(output_list[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "540 fits failed out of a total of 1620.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "173 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "367 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [            nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan -5.98151844e+08\n",
      " -6.00844463e+08 -5.96729551e+08 -5.95397727e+08 -5.90999870e+08\n",
      " -5.94984076e+08 -5.94943183e+08 -5.94648773e+08 -5.96943757e+08\n",
      " -5.88604049e+08 -5.94555434e+08 -5.96807916e+08 -5.89859485e+08\n",
      " -5.92729130e+08 -5.96283569e+08 -5.89109159e+08 -5.96113955e+08\n",
      " -5.99578221e+08 -5.98261731e+08 -5.95449607e+08 -5.96659047e+08\n",
      " -5.98261731e+08 -5.95449607e+08 -5.96659047e+08 -5.95391243e+08\n",
      " -5.95633163e+08 -5.98966226e+08 -6.16111817e+08 -6.04746670e+08\n",
      " -6.03428024e+08 -6.03317746e+08 -6.00727526e+08 -6.01422616e+08\n",
      " -6.06112695e+08 -6.04137039e+08 -6.06152163e+08 -6.15804016e+08\n",
      " -6.12744828e+08 -6.14041154e+08 -6.15041102e+08 -6.11851195e+08\n",
      " -6.14246604e+08 -6.19927124e+08 -6.15428908e+08 -6.14628899e+08\n",
      " -6.21038523e+08 -6.20993175e+08 -6.21872639e+08 -6.21038523e+08\n",
      " -6.20993175e+08 -6.21872639e+08 -6.22127889e+08 -6.21562889e+08\n",
      " -6.22245344e+08             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      " -6.16838128e+08 -6.13234512e+08 -6.16428246e+08 -6.20366012e+08\n",
      " -6.17451280e+08 -6.20343692e+08 -6.15809655e+08 -6.13284875e+08\n",
      " -6.17563369e+08 -6.08581718e+08 -6.13924864e+08 -6.17172945e+08\n",
      " -6.04727353e+08 -6.10359816e+08 -6.14575296e+08 -6.05839520e+08\n",
      " -6.10182594e+08 -6.15552930e+08 -6.10570062e+08 -6.08831020e+08\n",
      " -6.11685412e+08 -6.10570062e+08 -6.08831020e+08 -6.11685412e+08\n",
      " -6.09772780e+08 -6.08501457e+08 -6.12294856e+08 -6.46635886e+08\n",
      " -6.49024768e+08 -6.47215402e+08 -6.38781627e+08 -6.45246512e+08\n",
      " -6.47174627e+08 -6.43203313e+08 -6.46159828e+08 -6.46042889e+08\n",
      " -6.43091135e+08 -6.44960468e+08 -6.46738496e+08 -6.43299532e+08\n",
      " -6.44715502e+08 -6.45176408e+08 -6.43709307e+08 -6.45943273e+08\n",
      " -6.47100028e+08 -6.43109900e+08 -6.47090248e+08 -6.46855268e+08\n",
      " -6.43109900e+08 -6.47090248e+08 -6.46855268e+08 -6.42904323e+08\n",
      " -6.47011519e+08 -6.47528866e+08             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan -5.98552536e+08 -5.98237550e+08 -5.98357288e+08\n",
      " -6.00546513e+08 -6.00021969e+08 -6.03286163e+08 -6.05480894e+08\n",
      " -6.00667750e+08 -6.04192254e+08 -5.88790297e+08 -5.94609708e+08\n",
      " -6.00223727e+08 -5.91831440e+08 -5.97691216e+08 -6.00735204e+08\n",
      " -5.91401014e+08 -5.96786395e+08 -6.01635815e+08 -6.03350641e+08\n",
      " -6.01036096e+08 -6.02044182e+08 -6.03350641e+08 -6.01036096e+08\n",
      " -6.02044182e+08 -6.03731453e+08 -6.01113955e+08 -6.02468991e+08\n",
      " -6.15167845e+08 -6.16276234e+08 -6.16095674e+08 -6.22618193e+08\n",
      " -6.23267140e+08 -6.26082724e+08 -6.25351761e+08 -6.24542887e+08\n",
      " -6.25838420e+08 -6.25703167e+08 -6.23779692e+08 -6.28061103e+08\n",
      " -6.26382353e+08 -6.26291177e+08 -6.28915242e+08 -6.26549647e+08\n",
      " -6.26945107e+08 -6.29409497e+08 -6.33166376e+08 -6.32505241e+08\n",
      " -6.32105441e+08 -6.33166376e+08 -6.32505241e+08 -6.32105441e+08\n",
      " -6.31676419e+08 -6.31876440e+08 -6.32442643e+08             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan -6.03832157e+08 -5.99846599e+08\n",
      " -5.98971102e+08 -5.96099973e+08 -5.96430808e+08 -6.00020104e+08\n",
      " -5.96167970e+08 -5.96357468e+08 -5.98259798e+08 -5.85287138e+08\n",
      " -5.92927594e+08 -5.96281094e+08 -5.89127774e+08 -5.94468567e+08\n",
      " -5.97394930e+08 -5.93031765e+08 -5.98075944e+08 -6.01460130e+08\n",
      " -5.96485515e+08 -5.95865507e+08 -5.98646498e+08 -5.96485515e+08\n",
      " -5.95865507e+08 -5.98646498e+08 -5.99072339e+08 -5.96043524e+08\n",
      " -5.99669441e+08 -6.21689702e+08 -6.15375493e+08 -6.09603214e+08\n",
      " -6.15173286e+08 -6.15442889e+08 -6.15020198e+08 -6.17762547e+08\n",
      " -6.17682378e+08 -6.17007441e+08 -6.18245577e+08 -6.18478045e+08\n",
      " -6.21335940e+08 -6.20705659e+08 -6.19041105e+08 -6.21391870e+08\n",
      " -6.20461171e+08 -6.20614378e+08 -6.21775865e+08 -6.25981001e+08\n",
      " -6.24959844e+08 -6.25852792e+08 -6.25981001e+08 -6.24959844e+08\n",
      " -6.25852792e+08 -6.24860415e+08 -6.24524686e+08 -6.25850659e+08]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 30, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Initialize the regressor\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Best parameters and best model\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation set: 2819676604.812782\n",
      "                     username  predicted_like_count\n",
      "0              beyazyakaliyiz          10770.258789\n",
      "1  totalenergies_istasyonlari            453.646881\n",
      "2                 konforyatak            370.162567\n",
      "3                    ht_kulup            484.007294\n",
      "4                   ajansspor           2237.495117\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare the target variable: Average like count per user\n",
    "user_avg_likes = {}\n",
    "for username, posts in username2posts_train.items():\n",
    "    avg_likes = np.mean([post.get(\"like_count\", 0) or 0 for post in posts])\n",
    "    user_avg_likes[username] = avg_likes\n",
    "\n",
    "# Create the target array\n",
    "y_like_counts = [user_avg_likes[uname] for uname in train_usernames]\n",
    "y_like_counts = [0 if np.isnan(val) else val for val in y_like_counts]\n",
    "\n",
    "# Train-test split\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_post_train, y_like_counts, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train an MLP Regressor\n",
    "xgb_regressor = XGBRegressor(\n",
    "    n_estimators=100,   # Number of trees\n",
    "    learning_rate=0.1,  # Step size for updating weights\n",
    "    max_depth=6,        # Depth of each tree\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_regressor.fit(x_train, y_train)\n",
    "\n",
    "y_val = np.maximum(y_val, 0)\n",
    "\n",
    "\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred_val = xgb_regressor.predict(x_val)\n",
    "y_pred_val = np.maximum(y_pred_val, 0)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_val, y_pred_val)\n",
    "print(f\"MSE on validation set: {mse}\")\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = xgb_regressor.predict(x_post_test)\n",
    "\n",
    "\n",
    "# Create a DataFrame for test predictions\n",
    "test_predictions = pd.DataFrame({\n",
    "    \"username\": test_usernames,\n",
    "    \"predicted_like_count\": y_test_pred\n",
    "})\n",
    "print(test_predictions.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.121146 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 234722\n",
      "[LightGBM] [Info] Number of data points in the train set: 2192, number of used features: 9802\n",
      "[LightGBM] [Info] Start training from score 8.538830\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "MSE on validation set: 2894020216.598778\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "                     username  predicted_like_count\n",
      "0              beyazyakaliyiz           7534.642583\n",
      "1  totalenergies_istasyonlari            160.384542\n",
      "2                 konforyatak            385.247891\n",
      "3                    ht_kulup            508.651928\n",
      "4                   ajansspor            756.686563\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare the target variable: Average like count per user\n",
    "user_avg_likes = {}\n",
    "for username, posts in username2posts_train.items():\n",
    "    avg_likes = np.mean([post.get(\"like_count\", 0) or 0 for post in posts])\n",
    "    user_avg_likes[username] = avg_likes\n",
    "\n",
    "# Create the target array\n",
    "y_like_counts = [user_avg_likes[uname] for uname in train_usernames]\n",
    "y_like_counts = [0 if np.isnan(val) else val for val in y_like_counts]\n",
    "\n",
    "# Train-test split\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_post_train, y_like_counts, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "lgbm_regressor = LGBMRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=-1,  # No limit on depth\n",
    "    min_data_in_leaf=10,\n",
    "    objective=\"poisson\",  # Poisson objective for count data\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "lgbm_regressor.fit(x_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred_val = lgbm_regressor.predict(x_val)\n",
    "\n",
    "# Ensure non-negative predictions\n",
    "y_pred_val = np.maximum(y_pred_val, 0)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_val, y_pred_val)\n",
    "print(f\"MSE on validation set: {mse}\")\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = lgbm_regressor.predict(x_post_test)\n",
    "\n",
    "# Ensure non-negative predictions\n",
    "y_test_pred = np.maximum(y_test_pred, 0)\n",
    "\n",
    "# Create a DataFrame for test predictions\n",
    "test_predictions = pd.DataFrame({\n",
    "    \"username\": test_usernames,\n",
    "    \"predicted_like_count\": y_test_pred\n",
    "})\n",
    "\n",
    "print(test_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      username  predicted_like_count\n",
      "0               beyazyakaliyiz           7534.642583\n",
      "1   totalenergies_istasyonlari            160.384542\n",
      "2                  konforyatak            385.247891\n",
      "3                     ht_kulup            508.651928\n",
      "4                    ajansspor            756.686563\n",
      "5         yusufelibelediyesi08            261.366434\n",
      "6                     4bros.tr           1195.834232\n",
      "7              groundy.kadikoy           1229.219994\n",
      "8                drtubagunebak            873.349581\n",
      "9                   nihatcan11            287.458903\n",
      "10               mustafaaakcay            467.312584\n",
      "11                  mtmofamily           6641.780885\n",
      "12                 bonitoperde            506.482361\n",
      "13     balkanlardangelenlezzet            108.484971\n",
      "14      pedagoganne__gulozturk           2220.391098\n",
      "15                      iktm34            226.112602\n",
      "16                    yudumyag            369.639450\n",
      "17      bulentozdemir.edebiyat            383.583642\n",
      "18              imtolstoyevski           9080.643265\n",
      "19                 yorkkadikoy            173.121611\n",
      "20                  akkaalinda            375.899344\n",
      "21            mutfaktayusufvar            684.421545\n",
      "22                   antmodern            445.167233\n",
      "23                turkuazkablo            477.585085\n",
      "24             immergasturkiye            138.415375\n",
      "25              vanillaantalya           1703.881354\n",
      "26                  ermanyasar           3360.973360\n",
      "27                   tuna.food           1416.326194\n",
      "28        rustik.rus.restorani            124.054519\n",
      "29                    ilhansen           2437.464731\n",
      "30   institutfrancaisdeturquie            206.701871\n",
      "31      beachandbeyondswimwear           1528.377100\n",
      "32             monsternotebook            317.995437\n",
      "33                     bsynctr            200.468563\n",
      "34                     kbbzone            111.891020\n",
      "35                     dbbanyo            440.075474\n",
      "36          istanbulalpplastik           1697.432103\n",
      "37        enka_insaat_official            122.119646\n",
      "38              ozyufka.com_tr            475.149937\n",
      "39                nationalturk           1617.101047\n",
      "40             sametkaankuyucu           2481.802329\n",
      "41               tutbelediyesi            207.244179\n",
      "42                      odurla            248.831001\n",
      "43                   balagency            163.658001\n",
      "44              sokebelediyesi            265.884264\n",
      "45   dedemanpalandokenskilodge            278.668586\n",
      "46               binefismutfak           1460.442884\n",
      "47                tatli.sanati           1487.937924\n",
      "48            evimin.yemekleri            488.962138\n",
      "49                ceyo_turkiye            159.444472\n"
     ]
    }
   ],
   "source": [
    "print(test_predictions.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.isnan(y_val).any() or np.isnan(y_pred_val).any():\n",
    "    print(\"NaN values found in y_val or y_pred_val!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Log Errors: 1038.2862970820247\n"
     ]
    }
   ],
   "source": [
    "log_errors = np.log1p(y_val) - np.log1p(y_pred_val)  # Use np.log1p to handle zero values gracefully\n",
    "\n",
    "# Compute the absolute log errors\n",
    "abs_log_errors = np.abs(log_errors)\n",
    "\n",
    "# Sum of log errors\n",
    "sum_log_errors = np.sum(abs_log_errors)\n",
    "\n",
    "print(f\"Sum of Log Errors: {sum_log_errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation set: 2819676604.812782\n",
      "Sum of Log Errors: 1278.0898952402472\n",
      "                     username  predicted_like_count\n",
      "0              beyazyakaliyiz          10770.258789\n",
      "1  totalenergies_istasyonlari            453.646881\n",
      "2                 konforyatak            370.162567\n",
      "3                    ht_kulup            484.007294\n",
      "4                   ajansspor           2237.495117\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare the target variable: Average like count per user\n",
    "user_avg_likes = {}\n",
    "for username, posts in username2posts_train.items():\n",
    "    avg_likes = np.mean([post.get(\"like_count\", 0) or 0 for post in posts])\n",
    "    user_avg_likes[username] = avg_likes\n",
    "\n",
    "# Create the target array\n",
    "y_like_counts = [user_avg_likes[uname] for uname in train_usernames]\n",
    "y_like_counts = [0 if np.isnan(val) else val for val in y_like_counts]\n",
    "# Train-test split\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_post_train, y_like_counts, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "xgb_regressor = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    objective='reg:squarederror',  # Default; can change to 'reg:poisson' for count data\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_regressor.fit(x_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred_val = xgb_regressor.predict(x_val)\n",
    "\n",
    "# Post-process: Clamp negative predictions to 0\n",
    "y_pred_val = np.maximum(y_pred_val, 0)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_val, y_pred_val)\n",
    "log_errors = np.log1p(y_val) - np.log1p(y_pred_val)  # Use np.log1p to handle zero values\n",
    "abs_log_errors = np.abs(log_errors)\n",
    "sum_log_errors = np.sum(abs_log_errors)\n",
    "\n",
    "print(f\"MSE on validation set: {mse}\")\n",
    "print(f\"Sum of Log Errors: {sum_log_errors}\")\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = xgb_regressor.predict(x_post_test)\n",
    "\n",
    "# Post-process: Clamp negative predictions to 0\n",
    "y_test_pred = np.maximum(y_test_pred, 0)\n",
    "\n",
    "# Create a DataFrame for test predictions\n",
    "test_predictions = pd.DataFrame({\n",
    "    \"username\": test_usernames,\n",
    "    \"predicted_like_count\": y_test_pred\n",
    "})\n",
    "\n",
    "print(test_predictions.head())\n",
    "\n",
    "# Save test predictions to a CSV file\n",
    "#test_predictions.to_csv(\"test_predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Log Errors: 1278.0898952402472\n"
     ]
    }
   ],
   "source": [
    "log_errors = np.log1p(y_val) - np.log1p(y_pred_val)  # Use np.log1p to handle zero values gracefully\n",
    "\n",
    "# Compute the absolute log errors\n",
    "abs_log_errors = np.abs(log_errors)\n",
    "\n",
    "# Sum of log errors\n",
    "sum_log_errors = np.sum(abs_log_errors)\n",
    "\n",
    "print(f\"Sum of Log Errors: {sum_log_errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'user_avg_likes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(avg_likes):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN detected for user: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00musername\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, posts: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mposts\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43muser_avg_likes\u001b[49m[username] \u001b[38;5;241m=\u001b[39m avg_likes\n",
      "\u001b[1;31mNameError\u001b[0m: name 'user_avg_likes' is not defined"
     ]
    }
   ],
   "source": [
    "# Check for NaN in user_avg_likes\n",
    "for username, posts in username2posts_train.items():\n",
    "    avg_likes = np.mean([post.get(\"like_count\", 0) or 0 for post in posts])\n",
    "    if np.isnan(avg_likes):\n",
    "        print(f\"NaN detected for user: {username}, posts: {posts}\")\n",
    "    user_avg_likes[username] = avg_likes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_val_pred_reg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate log errors\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m log_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(y_val_reg \u001b[38;5;241m-\u001b[39m \u001b[43my_val_pred_reg\u001b[49m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Sum of log errors\u001b[39;00m\n\u001b[0;32m      5\u001b[0m sum_log_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(log_errors)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_val_pred_reg' is not defined"
     ]
    }
   ],
   "source": [
    "'''# Calculate log errors\n",
    "log_errors = np.abs(y_val_reg - y_val_pred_reg)\n",
    "\n",
    "# Sum of log errors\n",
    "sum_log_errors = np.sum(log_errors)\n",
    "\n",
    "print(\"Sum of Log Errors:\", sum_log_errors)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
