{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Osama\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "turkish_stopwords = stopwords.words('turkish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classification_df = pd.read_csv(\"/Users/Osama/Downloads/CS412PROJ/train-classification.csv\",)\n",
    "train_classification_df = train_classification_df.rename(columns={'Unnamed: 0': 'user_id', 'label': 'category'})\n",
    "\n",
    "# Unifying labels\n",
    "train_classification_df[\"category\"] = train_classification_df[\"category\"].apply(str.lower)\n",
    "username2_category = train_classification_df.set_index(\"user_id\").to_dict()[\"category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>art</th>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entertainment</th>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fashion</th>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>food</th>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gaming</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health and lifestyle</th>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mom and children</th>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sports</th>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tech</th>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>travel</th>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      user_id\n",
       "category                     \n",
       "art                       191\n",
       "entertainment             323\n",
       "fashion                   299\n",
       "food                      511\n",
       "gaming                     13\n",
       "health and lifestyle      503\n",
       "mom and children          149\n",
       "sports                    113\n",
       "tech                      346\n",
       "travel                    294"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_classification_df.groupby(\"category\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tech'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username2_category[\"kod8net\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"/Users/Osama/Downloads/CS412PROJ/training-dataset.jsonl.gz\"\n",
    "\n",
    "username2posts_train = dict()\n",
    "username2profile_train = dict()\n",
    "\n",
    "username2posts_test = dict()\n",
    "username2profile_test = dict()\n",
    "\n",
    "\n",
    "with gzip.open(train_data_path, \"rt\") as fh:\n",
    "  for line in fh:\n",
    "    sample = json.loads(line)\n",
    "\n",
    "    profile = sample[\"profile\"]\n",
    "    username = profile[\"username\"]\n",
    "    if username in username2_category:\n",
    "      # train data info\n",
    "      username2posts_train[username] = sample[\"posts\"]\n",
    "      username2profile_train[username] = profile\n",
    "\n",
    "\n",
    "    else:\n",
    "      # it is test data info\n",
    "      username2posts_test[username] = sample[\"posts\"]\n",
    "      username2profile_test[username] = profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2741, 44)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_profile_df = pd.DataFrame(username2profile_train).T.reset_index(drop=True)\n",
    "test_profile_df = pd.DataFrame(username2profile_test).T.reset_index(drop=True)\n",
    "\n",
    "train_profile_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>id</th>\n",
       "      <th>full_name</th>\n",
       "      <th>biography</th>\n",
       "      <th>category_name</th>\n",
       "      <th>post_count</th>\n",
       "      <th>follower_count</th>\n",
       "      <th>following_count</th>\n",
       "      <th>is_business_account</th>\n",
       "      <th>is_private</th>\n",
       "      <th>...</th>\n",
       "      <th>business_category_name</th>\n",
       "      <th>overall_category_name</th>\n",
       "      <th>category_enum</th>\n",
       "      <th>is_verified_by_mv4b</th>\n",
       "      <th>is_regulated_c18</th>\n",
       "      <th>profile_pic_url</th>\n",
       "      <th>should_show_category</th>\n",
       "      <th>should_show_public_contacts</th>\n",
       "      <th>show_account_transparency_details</th>\n",
       "      <th>profile_picture_base64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beyazyakaliyiz</td>\n",
       "      <td>8634457436</td>\n",
       "      <td>Selam Beyaz Yakalı</td>\n",
       "      <td>Beyaz yakalıların dünyasına hoşgeldiniz 😀😀😀</td>\n",
       "      <td>Personal blog</td>\n",
       "      <td>None</td>\n",
       "      <td>1265</td>\n",
       "      <td>665</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>PERSONAL_BLOG</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://instagram.fist6-1.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>totalenergies_istasyonlari</td>\n",
       "      <td>7066643793</td>\n",
       "      <td>TotalEnergies İstasyonları</td>\n",
       "      <td>TotalEnergies İstasyonları resmi Instagram hes...</td>\n",
       "      <td>Energy Company</td>\n",
       "      <td>None</td>\n",
       "      <td>28025</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ENERGY_COMPANY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://instagram.fsaw2-1.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>konforyatak</td>\n",
       "      <td>8782109673</td>\n",
       "      <td>Konfor Yatak #KonforluUykular</td>\n",
       "      <td>Konfor Yatak, birbirinden farklı özelliklere s...</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>None</td>\n",
       "      <td>40334</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>FURNITURE</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://instagram.fyei6-3.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ht_kulup</td>\n",
       "      <td>1950140344</td>\n",
       "      <td>HT KULÜP</td>\n",
       "      <td>Bloomberght - Habertürk Magazin</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>158877</td>\n",
       "      <td>69</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>TV_SHOW</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://instagram.fada2-1.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ajansspor</td>\n",
       "      <td>338611487</td>\n",
       "      <td>Ajansspor</td>\n",
       "      <td>🏢 Saran Group \\n🏟 Anında, tarafsız spor haberl...</td>\n",
       "      <td>News &amp; media website</td>\n",
       "      <td>None</td>\n",
       "      <td>93193</td>\n",
       "      <td>286</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NEWS_SITE</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://instagram.fadb2-1.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>yusufelibelediyesi08</td>\n",
       "      <td>43824173432</td>\n",
       "      <td>Yusufeli Belediye Başkanlığı</td>\n",
       "      <td>Yusufeli Belediye Başkanlığı Resmi İnstagram H...</td>\n",
       "      <td>Community</td>\n",
       "      <td>None</td>\n",
       "      <td>4635</td>\n",
       "      <td>83</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://instagram.fszf2-1.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4bros.tr</td>\n",
       "      <td>8598418995</td>\n",
       "      <td>4 Bros Burger</td>\n",
       "      <td>📍Özlüce/Bursa\\nAhmet Taner Kışlalı Bulv.\\n📞0 2...</td>\n",
       "      <td>Restaurant</td>\n",
       "      <td>None</td>\n",
       "      <td>5824</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>RESTAURANT</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://instagram.fadb3-1.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>groundy.kadikoy</td>\n",
       "      <td>5357204834</td>\n",
       "      <td>Groundy Kadıköy</td>\n",
       "      <td>📲 0530 434 3535\\nGroundy Kadıköy Güncel Etkinl...</td>\n",
       "      <td>Dance &amp; Night Club</td>\n",
       "      <td>None</td>\n",
       "      <td>14874</td>\n",
       "      <td>17</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NIGHT_CLUB</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://instagram.fadb2-2.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>drtubagunebak</td>\n",
       "      <td>1118780766</td>\n",
       "      <td>Dr. Ç.Tuba Günebak</td>\n",
       "      <td>👩🏼‍⚕️Ankara ofis:Beytepe\\n👩🏼‍💼İstanbul ofis:Ba...</td>\n",
       "      <td>Nutritionist</td>\n",
       "      <td>None</td>\n",
       "      <td>44367</td>\n",
       "      <td>3600</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NUTRITIONIST</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://instagram.fesb4-1.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nihatcan11</td>\n",
       "      <td>7728929668</td>\n",
       "      <td>Nihat Can</td>\n",
       "      <td>None</td>\n",
       "      <td>Politician</td>\n",
       "      <td>None</td>\n",
       "      <td>2749</td>\n",
       "      <td>254</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>POLITICIAN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://instagram.fesb7-1.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     username           id                      full_name  \\\n",
       "0              beyazyakaliyiz   8634457436             Selam Beyaz Yakalı   \n",
       "1  totalenergies_istasyonlari   7066643793    TotalEnergies İstasyonları   \n",
       "2                 konforyatak   8782109673  Konfor Yatak #KonforluUykular   \n",
       "3                    ht_kulup   1950140344                       HT KULÜP   \n",
       "4                   ajansspor    338611487                      Ajansspor   \n",
       "5        yusufelibelediyesi08  43824173432   Yusufeli Belediye Başkanlığı   \n",
       "6                    4bros.tr   8598418995                  4 Bros Burger   \n",
       "7             groundy.kadikoy   5357204834                Groundy Kadıköy   \n",
       "8               drtubagunebak   1118780766             Dr. Ç.Tuba Günebak   \n",
       "9                  nihatcan11   7728929668                      Nihat Can   \n",
       "\n",
       "                                           biography         category_name  \\\n",
       "0        Beyaz yakalıların dünyasına hoşgeldiniz 😀😀😀         Personal blog   \n",
       "1  TotalEnergies İstasyonları resmi Instagram hes...        Energy Company   \n",
       "2  Konfor Yatak, birbirinden farklı özelliklere s...             Furniture   \n",
       "3                    Bloomberght - Habertürk Magazin                  None   \n",
       "4  🏢 Saran Group \\n🏟 Anında, tarafsız spor haberl...  News & media website   \n",
       "5  Yusufeli Belediye Başkanlığı Resmi İnstagram H...             Community   \n",
       "6  📍Özlüce/Bursa\\nAhmet Taner Kışlalı Bulv.\\n📞0 2...            Restaurant   \n",
       "7  📲 0530 434 3535\\nGroundy Kadıköy Güncel Etkinl...    Dance & Night Club   \n",
       "8  👩🏼‍⚕️Ankara ofis:Beytepe\\n👩🏼‍💼İstanbul ofis:Ba...          Nutritionist   \n",
       "9                                               None            Politician   \n",
       "\n",
       "  post_count follower_count following_count is_business_account is_private  \\\n",
       "0       None           1265             665                True      False   \n",
       "1       None          28025               4                True      False   \n",
       "2       None          40334               2                True      False   \n",
       "3       None         158877              69                True      False   \n",
       "4       None          93193             286                True      False   \n",
       "5       None           4635              83                True      False   \n",
       "6       None           5824               5                True      False   \n",
       "7       None          14874              17                True      False   \n",
       "8       None          44367            3600                True      False   \n",
       "9       None           2749             254                True      False   \n",
       "\n",
       "   ... business_category_name overall_category_name   category_enum  \\\n",
       "0  ...                   None                  None   PERSONAL_BLOG   \n",
       "1  ...                   None                  None  ENERGY_COMPANY   \n",
       "2  ...                   None                  None       FURNITURE   \n",
       "3  ...                   None                  None         TV_SHOW   \n",
       "4  ...                   None                  None       NEWS_SITE   \n",
       "5  ...                   None                  None            None   \n",
       "6  ...                   None                  None      RESTAURANT   \n",
       "7  ...                   None                  None      NIGHT_CLUB   \n",
       "8  ...                   None                  None    NUTRITIONIST   \n",
       "9  ...                   None                  None      POLITICIAN   \n",
       "\n",
       "  is_verified_by_mv4b is_regulated_c18  \\\n",
       "0               False            False   \n",
       "1               False            False   \n",
       "2               False            False   \n",
       "3               False            False   \n",
       "4               False            False   \n",
       "5               False            False   \n",
       "6               False            False   \n",
       "7               False            False   \n",
       "8               False            False   \n",
       "9               False            False   \n",
       "\n",
       "                                     profile_pic_url should_show_category  \\\n",
       "0  https://instagram.fist6-1.fna.fbcdn.net/v/t51....                 True   \n",
       "1  https://instagram.fsaw2-1.fna.fbcdn.net/v/t51....                 True   \n",
       "2  https://instagram.fyei6-3.fna.fbcdn.net/v/t51....                 True   \n",
       "3  https://instagram.fada2-1.fna.fbcdn.net/v/t51....                 True   \n",
       "4  https://instagram.fadb2-1.fna.fbcdn.net/v/t51....                 True   \n",
       "5  https://instagram.fszf2-1.fna.fbcdn.net/v/t51....                False   \n",
       "6  https://instagram.fadb3-1.fna.fbcdn.net/v/t51....                 True   \n",
       "7  https://instagram.fadb2-2.fna.fbcdn.net/v/t51....                 True   \n",
       "8  https://instagram.fesb4-1.fna.fbcdn.net/v/t51....                 True   \n",
       "9  https://instagram.fesb7-1.fna.fbcdn.net/v/t51....                 True   \n",
       "\n",
       "  should_show_public_contacts show_account_transparency_details  \\\n",
       "0                        True                              True   \n",
       "1                        True                              True   \n",
       "2                        True                              True   \n",
       "3                        True                              True   \n",
       "4                        True                              True   \n",
       "5                       False                              True   \n",
       "6                        True                              True   \n",
       "7                        True                              True   \n",
       "8                        True                              True   \n",
       "9                        True                              True   \n",
       "\n",
       "                              profile_picture_base64  \n",
       "0  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
       "1  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
       "2  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
       "3  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
       "4  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
       "5  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
       "6  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
       "7  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
       "8  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
       "9  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
       "\n",
       "[10 rows x 44 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_profile_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the              43.234850\n",
      "olarak           40.352663\n",
      "kutlu            38.096409\n",
      "kutlu olsun      37.759918\n",
      "güzel            37.389047\n",
      "and              34.504780\n",
      "of               34.106894\n",
      "to               33.099203\n",
      "birlikte         32.901064\n",
      "in               32.374707\n",
      "with             28.743966\n",
      "bilgi            28.325731\n",
      "teşekkür         28.247761\n",
      "mustafa          27.676218\n",
      "for              27.280477\n",
      "türkiye          27.194764\n",
      "you              26.948208\n",
      "kemal            26.702591\n",
      "is               25.541688\n",
      "mustafa kemal    25.536023\n",
      "mi               25.390266\n",
      "ziyaret          25.182333\n",
      "olmak            24.740089\n",
      "ben              24.492370\n",
      "sonra            24.243549\n",
      "hafta            24.095780\n",
      "lezzet           23.966902\n",
      "bekliyor         23.044388\n",
      "bekliyoruz       22.340900\n",
      "our              22.187366\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import emoji\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "def preprocess_text(text: str):\n",
    "    \n",
    "\n",
    "    # lower casing Turkish Text, Don't use str.lower :)\n",
    "    text = text.casefold()\n",
    "\n",
    "    #text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove special characters and punctuation\n",
    "    # HERE THE EMOJIS stuff are being removed, you may want to keep them :D\n",
    "    text = re.sub(r'[^a-zçğıöşü0-9\\s#@]', '', text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "corpus = []\n",
    "\n",
    "# to keep the label order\n",
    "train_usernames = []\n",
    "\n",
    "for username, posts in username2posts_train.items():\n",
    "  train_usernames.append(username)\n",
    "\n",
    "  # aggregating the posts per user\n",
    "  cleaned_captions = []\n",
    "  for post in posts:\n",
    "    post_caption = post.get(\"caption\", \"\")\n",
    "    if post_caption is None:\n",
    "      continue\n",
    "\n",
    "    post_caption = preprocess_text(post_caption)\n",
    "\n",
    "    if post_caption != \"\":\n",
    "      cleaned_captions.append(post_caption)\n",
    "\n",
    "\n",
    "  # joining the posts of each user with a \\n\n",
    "  user_post_captions = \"\\n\".join(cleaned_captions)\n",
    "  corpus.append(user_post_captions)\n",
    "\n",
    "\n",
    "\n",
    "#custom_stopwords = list(set(turkish_stopwords).union({\n",
    " #   'the', 'and', 'with', 'for', 'you', 'to', 'of', 'in', 'our', 'your', 'is', 'are','bir'\n",
    "#}))\n",
    "vectorizer = TfidfVectorizer(stop_words=turkish_stopwords, max_features=15000,min_df=10,sublinear_tf=True,smooth_idf=False,ngram_range=(1, 3))\n",
    "\n",
    "# fit the vectorizer\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "# transform the data into vectors\n",
    "x_post_train = vectorizer.transform(corpus)\n",
    "y_train = [username2_category.get(uname, \"NA\") for uname in train_usernames]\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Perform chi-squared feature selection\n",
    "chi2_selector = SelectKBest(chi2, k=5000)  # Select the top 5000 features\n",
    "x_post_train_selected = chi2_selector.fit_transform(x_post_train, y_train)\n",
    "\n",
    "# Get the selected feature names\n",
    "selected_feature_indices = chi2_selector.get_support(indices=True)\n",
    "selected_feature_names = np.array(feature_names)[selected_feature_indices]\n",
    "\n",
    "\n",
    "\n",
    "# Convert the selected features into a DataFrame for analysis\n",
    "df_tfidf = pd.DataFrame(x_post_train_selected.toarray(), columns=selected_feature_names)\n",
    "\n",
    "# Show the most frequent selected words\n",
    "print(df_tfidf.sum().sort_values(ascending=False).head(30))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "# Inspect the frequency of each word\n",
    "df_tfidf = pd.DataFrame(x_post_train.toarray(), columns=feature_names)\n",
    "\n",
    "# Show the most frequent words (words in many posts)\n",
    "print(df_tfidf.sum().sort_values(ascending=False).head(30))\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "test_usernames = []\n",
    "test_corpus = []\n",
    "for username, posts in username2posts_test.items():\n",
    "  test_usernames.append(username)\n",
    "  # aggregating the posts per user\n",
    "  cleaned_captions = []\n",
    "  for post in posts:\n",
    "    post_caption = post.get(\"caption\", \"\")\n",
    "    if post_caption is None:\n",
    "      continue\n",
    "\n",
    "    post_caption = preprocess_text(post_caption)\n",
    "\n",
    "    if post_caption != \"\":\n",
    "      cleaned_captions.append(post_caption)\n",
    "\n",
    "  user_post_captions = \"\\n\".join(cleaned_captions)\n",
    "  test_corpus.append(user_post_captions)\n",
    "\n",
    "\n",
    "# Just transforming! No Fitting!!!!!\n",
    "x_post_test = vectorizer.transform(test_corpus)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Transform the test set using the same feature selection\n",
    "x_post_test_selected = chi2_selector.transform(x_post_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure everything is fine\n",
    "assert y_train.count(\"NA\") == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ab', 'abant', 'abd', ..., 'şıklığın', 'şımartın', 'şırnak'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>abant</th>\n",
       "      <th>abd</th>\n",
       "      <th>abdulkadir</th>\n",
       "      <th>abdullah</th>\n",
       "      <th>abi</th>\n",
       "      <th>ability</th>\n",
       "      <th>abiye</th>\n",
       "      <th>able</th>\n",
       "      <th>able to</th>\n",
       "      <th>...</th>\n",
       "      <th>şükürler</th>\n",
       "      <th>şükürler olsun</th>\n",
       "      <th>şüphesiz</th>\n",
       "      <th>şık</th>\n",
       "      <th>şık bir</th>\n",
       "      <th>şıklık</th>\n",
       "      <th>şıklığı</th>\n",
       "      <th>şıklığın</th>\n",
       "      <th>şımartın</th>\n",
       "      <th>şırnak</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04435</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 15000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ab  abant  abd  abdulkadir  abdullah  abi  ability  abiye  able  able to  \\\n",
       "0  0.0    0.0  0.0         0.0       0.0  0.0      0.0    0.0   0.0      0.0   \n",
       "1  0.0    0.0  0.0         0.0       0.0  0.0      0.0    0.0   0.0      0.0   \n",
       "\n",
       "   ...  şükürler  şükürler olsun  şüphesiz      şık  şık bir  şıklık  şıklığı  \\\n",
       "0  ...       0.0             0.0       0.0  0.04435      0.0     0.0      0.0   \n",
       "1  ...       0.0             0.0       0.0  0.00000      0.0     0.0      0.0   \n",
       "\n",
       "   şıklığın  şımartın  şırnak  \n",
       "0       0.0       0.0     0.0  \n",
       "1       0.0       0.0     0.0  \n",
       "\n",
       "[2 rows x 15000 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf = pd.DataFrame(x_post_train.toarray(), columns=feature_names)\n",
    "df_tfidf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2741, 15000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df_tfidf: (2741, 15000)\n",
      "Length of y_train: 2741\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of df_tfidf:\", df_tfidf.shape)\n",
    "print(\"Length of y_train:\", len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(df_tfidf, y_train, test_size=0.2, stratify=y_train,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2192, 15000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(549, 15000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ozhotelstr\n",
      "elleturkiye\n",
      "sozerinsaatorhangazi\n",
      "sanliurfapiazzaavym\n",
      "rusanozden\n",
      "*****\n",
      "['ozhotelstr', 'elleturkiye', 'sozerinsaatorhangazi', 'sanliurfapiazzaavym', 'rusanozden']\n"
     ]
    }
   ],
   "source": [
    "#@title Test Data\n",
    "test_data_path = \"/Users/Osama/Downloads/CS412PROJ/test-classification-round1.dat\"\n",
    "\n",
    "with open(test_data_path, \"rt\") as fh:\n",
    "    for i, line in enumerate(fh):\n",
    "        print(line.strip())\n",
    "        if i >= 4:  # Stop after 5 lines\n",
    "            break\n",
    "\n",
    "print(\"*****\")\n",
    "\n",
    "test_unames = []\n",
    "with open(test_data_path, \"rt\") as fh:\n",
    "  for line in fh:\n",
    "    test_unames.append(line.strip())\n",
    "\n",
    "print(test_unames[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "screenname\n"
     ]
    }
   ],
   "source": [
    "x_test = []\n",
    "\n",
    "for uname in test_unames:\n",
    "  try:\n",
    "    index = test_usernames.index(uname)\n",
    "    x_test.append(x_post_test[index].toarray()[0])\n",
    "  except Exception as e:\n",
    "    try:\n",
    "      index = train_usernames.index(uname)\n",
    "      x_test.append(x_post_train[index].toarray()[0])\n",
    "    except Exception as e:\n",
    "      print(uname)\n",
    "\n",
    "\n",
    "test_unames.remove(\"screenname\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>abant</th>\n",
       "      <th>abd</th>\n",
       "      <th>abdulkadir</th>\n",
       "      <th>abdullah</th>\n",
       "      <th>abi</th>\n",
       "      <th>ability</th>\n",
       "      <th>abiye</th>\n",
       "      <th>able</th>\n",
       "      <th>able to</th>\n",
       "      <th>...</th>\n",
       "      <th>şükürler</th>\n",
       "      <th>şükürler olsun</th>\n",
       "      <th>şüphesiz</th>\n",
       "      <th>şık</th>\n",
       "      <th>şık bir</th>\n",
       "      <th>şıklık</th>\n",
       "      <th>şıklığı</th>\n",
       "      <th>şıklığın</th>\n",
       "      <th>şımartın</th>\n",
       "      <th>şırnak</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027864</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031456</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 15000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ab  abant       abd  abdulkadir  abdullah  abi  ability  abiye  able  \\\n",
       "0  0.0    0.0  0.000000         0.0       0.0  0.0      0.0    0.0   0.0   \n",
       "1  0.0    0.0  0.027864         0.0       0.0  0.0      0.0    0.0   0.0   \n",
       "\n",
       "   able to  ...  şükürler  şükürler olsun  şüphesiz       şık  şık bir  \\\n",
       "0      0.0  ...       0.0             0.0       0.0  0.000000      0.0   \n",
       "1      0.0  ...       0.0             0.0       0.0  0.031456      0.0   \n",
       "\n",
       "   şıklık  şıklığı  şıklığın  şımartın  şırnak  \n",
       "0     0.0      0.0       0.0       0.0     0.0  \n",
       "1     0.0      0.0       0.0       0.0     0.0  \n",
       "\n",
       "[2 rows x 15000 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame(np.array(x_test), columns=feature_names)\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_like_count(username, current_post=None):\n",
    "  def get_avg_like_count(posts:list):\n",
    "    total = 0.\n",
    "    for post in posts:\n",
    "      if current_post is not None and post[\"id\"] == current_post[\"id\"]:\n",
    "        continue\n",
    "\n",
    "      like_count = post.get(\"like_count\", 0)\n",
    "      if like_count is None:\n",
    "        like_count = 0\n",
    "      total += like_count\n",
    "\n",
    "    if len(posts) == 0:\n",
    "      return 0.\n",
    "\n",
    "    return total / len(posts)\n",
    "\n",
    "  if username in username2posts_train:\n",
    "    return get_avg_like_count(username2posts_train[username])\n",
    "  elif username in username2posts_test:\n",
    "    return get_avg_like_count(username2posts_test[username])\n",
    "  else:\n",
    "    print(f\"No data available for {username}\")\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_mse_like_counts(y_true, y_pred):\n",
    "  \"\"\"\n",
    "  Calculate the Log Mean Squared Error (Log MSE) for like counts (log(like_count + 1)).\n",
    "\n",
    "  Parameters:\n",
    "  - y_true: array-like, actual like counts\n",
    "  - y_pred: array-like, predicted like counts\n",
    "\n",
    "  Returns:\n",
    "  - log_mse: float, Log Mean Squared Error\n",
    "  \"\"\"\n",
    "  # Ensure inputs are numpy arrays\n",
    "  y_true = np.array(y_true)\n",
    "  y_pred = np.array(y_pred)\n",
    "\n",
    "  # Log transformation: log(like_count + 1)\n",
    "  log_y_true = np.log1p(y_true)\n",
    "  log_y_pred = np.log1p(y_pred)\n",
    "\n",
    "  # Compute squared errors\n",
    "  squared_errors = (log_y_true - log_y_pred) ** 2\n",
    "\n",
    "  # Return the mean of squared errors\n",
    "  return np.mean(squared_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log MSE Train= 1.2271047744059362\n"
     ]
    }
   ],
   "source": [
    "#@title Train Dataset evaluation\n",
    "\n",
    "y_like_count_train_true = []\n",
    "y_like_count_train_pred = []\n",
    "for uname, posts in username2posts_train.items():\n",
    "  for post in posts:\n",
    "    pred_val = predict_like_count(uname, post)\n",
    "    true_val = post.get(\"like_count\", 0)\n",
    "    if true_val is None:\n",
    "      true_val = 0\n",
    "\n",
    "    y_like_count_train_true.append(true_val)\n",
    "    y_like_count_train_pred.append(pred_val)\n",
    "\n",
    "print(f\"Log MSE Train= {log_mse_like_counts(y_like_count_train_true, y_like_count_train_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Test Dataset\n",
    "\n",
    "path = \"/Users/Osama/Downloads/CS412PROJ/test-regression-round1.jsonl\"\n",
    "output_path = \"/Users/Osama/Downloads/CS412PROJ/testy-regression-round1.jsonl\"\n",
    "\n",
    "to_predict_like_counts_usernames = []\n",
    "output_list = []\n",
    "with open(path, \"rt\") as fh:\n",
    "  for line in fh:\n",
    "    sample = json.loads(line)\n",
    "\n",
    "    # let's predict\n",
    "    pred_val = predict_like_count(sample[\"username\"])\n",
    "    sample[\"like_count\"] = int(pred_val)\n",
    "    output_list.append(sample)\n",
    "\n",
    "with open(output_path, \"wt\") as of:\n",
    "  json.dump(output_list, of)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'caption': 'KOZA 2023 2.si Damla’nın koleksiyonu, Latincede ‘Memento Mori’ '\n",
      "             'olarak bilinen ‘ölümlü olduğunu hatırla’ anlamındaki ifadeden '\n",
      "             'esinleniyor. Koleksiyon, hayatın ve ölümün, para, işçi, kral ve '\n",
      "             'kraliçe kavramları üzerinden yaratıcı görünümlerle bir araya '\n",
      "             'getirilmesini amaçlıyor. Ölüm sembollerinden esinlenen desenler '\n",
      "             'kullanan Damla, “kağıt parçasından ibaret olmak” kavramını '\n",
      "             'vurguluyor. Koleksiyon, yaşamın ve ölümün aynı anda ifade '\n",
      "             'edilmesini hedefliyor; kırmızı ve mavi ışıklarla veya '\n",
      "             'gözlüklerle görülen hologram efekti kullanılarak bu konsept '\n",
      "             'sahneye taşınıyor. Kırmızı renk ölümü, mavi ise yaşamı '\n",
      "             'simgeliyor. Koleksiyon, ofis giyimlerinden esinlenerek '\n",
      "             'kravatlar, gömlekler ve evrak çantaları içeriyor. Klasik sivri '\n",
      "             'burun çizmelerin üzerine spor ayakkabıların üst yüzeyi '\n",
      "             'yerleştirilerek, iş dünyasının koşuşturması ve cenaze '\n",
      "             'temalarının aynı anda ifade edilmesi amaçlanıyor. Para kazanma '\n",
      "             'arzusu, kırmızı zambak desenleri ve büyük mücevher görünümleri '\n",
      "             'ile koleksiyon tamamlanıyor.\\n'\n",
      "             '\\n'\n",
      "             'Tebrikler Damla!\\n'\n",
      "             '\\n'\n",
      "             '#GencModaTasarimcilari #Koza2023 #KozaYarismasi '\n",
      "             '#TasarimYarismasi #Moda #Fashion #ModaTasarımı',\n",
      "  'comments_count': 2,\n",
      "  'id': '18144550534306740',\n",
      "  'like_count': 158,\n",
      "  'media_type': 'CAROUSEL_ALBUM',\n",
      "  'media_url': 'https://scontent-sof1-1.cdninstagram.com/v/t51.29350-15/397997154_1016992459537522_4925783512176260397_n.jpg?_nc_cat=106&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=7V_eObkFeK4AX-LMtsK&_nc_ht=scontent-sof1-1.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfDEqDhzaTO3ezV-veT6cJFCOcAEyeVzHR6si9n33N6G5A&oe=6551B6B9',\n",
      "  'timestamp': '2023-11-02 15:49:22',\n",
      "  'username': 'kozayarismasi'},\n",
      " {'caption': 'Tüm Türkiye ve Avrupa’ya sevkiyatlarımız aralıksız devam ediyor! '\n",
      "             'Aracımız Bursa’dan Ordu’ya müşterimizin ürünleri için yola '\n",
      "             'çıkıyor.\\n'\n",
      "             '\\n'\n",
      "             '👉Tuna Mah. Etibank Cad. No:134 Osmangazi/BURSA\\n'\n",
      "             '\\n'\n",
      "             'www.celikbeymobilya.com sitemizden tüm modelleri '\n",
      "             'inceleyebilirsiniz. \\n'\n",
      "             '\\n'\n",
      "             '#bursa #almanya #fransa',\n",
      "  'comments_count': 0,\n",
      "  'id': '17995331788956693',\n",
      "  'like_count': 99,\n",
      "  'media_type': 'VIDEO',\n",
      "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m82/BF4767CB85BDFB8ADCCCA8F15B8C20B5_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmNsaXBzLnVua25vd24tQzMuNzIwLmRhc2hfYmFzZWxpbmVfMV92MSJ9&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=110&vs=1259525061418244_1441854817&_nc_vs=HBksFQIYT2lnX3hwdl9yZWVsc19wZXJtYW5lbnRfcHJvZC9CRjQ3NjdDQjg1QkRGQjhBRENDQ0E4RjE1QjhDMjBCNV92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dBRWdfaFZfcDVCYk5HZ0NBQTlzVURvZW5mZ3FicV9FQUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJvS3uOiQ0P8%2FFQIoAkMzLBdAJO%2Bdsi0OVhgSZGFzaF9iYXNlbGluZV8xX3YxEQB1AAA%3D&ccb=9-4&oh=00_AfAm22JssMPaUlQe3rpYsFWBhFb5mUgolTCdhV0Xgm4AnA&oe=6556A482&_nc_sid=1d576d&_nc_rid=cd9a998e44',\n",
      "  'timestamp': '2023-08-19 13:46:02',\n",
      "  'username': 'celikbeymobilya'},\n",
      " {'caption': '🤩\\n'\n",
      "             '\\n'\n",
      "             '.\\n'\n",
      "             '.\\n'\n",
      "             'Daha Fazlası İçin Beğenmeyi ve Takip Etmeyi Unutmayın\\n'\n",
      "             '.\\n'\n",
      "             'girisimci_muhendis ✅\\n'\n",
      "             '.\\n'\n",
      "             '📣Bu Bilgi Hakkında Ne Düşünüyorsunuz.\\n'\n",
      "             '.\\n'\n",
      "             '✅Görmesini İstediğin Arkadaşını Etiketle.\\n'\n",
      "             '.\\n'\n",
      "             '🔔Gönderi Bildirimlerini Açarak, Bilgileri Anında '\n",
      "             'Öğrenebilirsiniz.\\n'\n",
      "             '\\n'\n",
      "             '.\\n'\n",
      "             '\\n'\n",
      "             'Source: Unknown\\n'\n",
      "             'Dm for Credit or removal\\n'\n",
      "             '.\\n'\n",
      "             '.............................................................\\n'\n",
      "             'All rights and credits reserved to the respective owner(s). If '\n",
      "             'you are the main copyright owner rather than the one mentioned '\n",
      "             'here of this content, contact me to claim credit or content '\n",
      "             'removal',\n",
      "  'comments_count': 75,\n",
      "  'id': '18302703232191518',\n",
      "  'like_count': 1224,\n",
      "  'media_type': 'VIDEO',\n",
      "  'media_url': None,\n",
      "  'timestamp': '2023-10-02 06:53:33',\n",
      "  'username': 'girisimci_muhendis'}]\n"
     ]
    }
   ],
   "source": [
    "pprint(output_list[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.model_selection import GridSearchCV\\nfrom sklearn.ensemble import RandomForestRegressor\\n\\n\\n# Define the parameter grid\\nparam_grid = {\\n    \\'n_estimators\\': [50, 100, 200],\\n    \\'max_depth\\': [None, 10, 20, 30],\\n    \\'min_samples_split\\': [2, 5, 10],\\n    \\'min_samples_leaf\\': [1, 2, 4],\\n    \\'max_features\\': [\\'auto\\', \\'sqrt\\', \\'log2\\']\\n}\\n\\n# Initialize the regressor\\nrf = RandomForestRegressor(random_state=42)\\n\\n# Perform grid search\\ngrid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring=\\'neg_mean_squared_error\\', verbose=2, n_jobs=-1)\\ngrid_search.fit(x_train, y_train)\\n\\n# Best parameters and best model\\nprint(\"Best Parameters:\", grid_search.best_params_)\\nbest_model = grid_search.best_estimator_\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Initialize the regressor\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Best parameters and best model\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation set: 2844503843.9501276\n",
      "                     username  predicted_like_count\n",
      "0              beyazyakaliyiz           6031.495117\n",
      "1  totalenergies_istasyonlari            333.563660\n",
      "2                 konforyatak            333.563660\n",
      "3                    ht_kulup            318.940491\n",
      "4                   ajansspor            961.202271\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare the target variable: Average like count per user\n",
    "user_avg_likes = {}\n",
    "for username, posts in username2posts_train.items():\n",
    "    avg_likes = np.mean([post.get(\"like_count\", 0) or 0 for post in posts])\n",
    "    user_avg_likes[username] = avg_likes\n",
    "\n",
    "# Create the target array\n",
    "y_like_counts = [user_avg_likes[uname] for uname in train_usernames]\n",
    "y_like_counts = [0 if np.isnan(val) else val for val in y_like_counts]\n",
    "\n",
    "# Train-test split\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_post_train, y_like_counts, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train an MLP Regressor\n",
    "xgb_regressor = XGBRegressor(\n",
    "    n_estimators=100,   # Number of trees\n",
    "    learning_rate=0.1,  # Step size for updating weights\n",
    "    max_depth=6,        # Depth of each tree\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_regressor.fit(x_train, y_train)\n",
    "\n",
    "y_val = np.maximum(y_val, 0)\n",
    "\n",
    "\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred_val = xgb_regressor.predict(x_val)\n",
    "y_pred_val = np.maximum(y_pred_val, 0)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_val, y_pred_val)\n",
    "print(f\"MSE on validation set: {mse}\")\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = xgb_regressor.predict(x_post_test)\n",
    "\n",
    "\n",
    "# Create a DataFrame for test predictions\n",
    "test_predictions = pd.DataFrame({\n",
    "    \"username\": test_usernames,\n",
    "    \"predicted_like_count\": y_test_pred\n",
    "})\n",
    "print(test_predictions.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.090126 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 212264\n",
      "[LightGBM] [Info] Number of data points in the train set: 2192, number of used features: 7003\n",
      "[LightGBM] [Info] Start training from score 8.538830\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "MSE on validation set: 2.975259638495896\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "                     username  predicted_like_count\n",
      "0              beyazyakaliyiz           6558.287578\n",
      "1  totalenergies_istasyonlari             20.860497\n",
      "2                 konforyatak             22.142566\n",
      "3                    ht_kulup            188.599836\n",
      "4                   ajansspor            762.109647\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare the target variable: Average like count per user\n",
    "user_avg_likes = {}\n",
    "for username, posts in username2posts_train.items():\n",
    "    avg_likes = np.mean([post.get(\"like_count\", 0) or 0 for post in posts])\n",
    "    user_avg_likes[username] = avg_likes\n",
    "\n",
    "# Create the target array\n",
    "y_like_counts = [user_avg_likes[uname] for uname in train_usernames]\n",
    "y_like_counts = [0 if np.isnan(val) else val for val in y_like_counts]\n",
    "\n",
    "# Train-test split\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_post_train, y_like_counts, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "lgbm_regressor = LGBMRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.2,\n",
    "    max_depth=30,\n",
    "    min_data_in_leaf=30,\n",
    "    feature_fraction=0.8,\n",
    "    bagging_fraction=0.8,\n",
    "    objective=\"poisson\",  # Poisson objective for count data\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "lgbm_regressor.fit(x_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred_val = lgbm_regressor.predict(x_val)\n",
    "\n",
    "# Ensure non-negative predictions\n",
    "y_pred_val = np.maximum(y_pred_val, 0)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = log_mse_like_counts(y_val, y_pred_val)\n",
    "print(f\"MSE on validation set: {mse}\")\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = lgbm_regressor.predict(x_post_test)\n",
    "\n",
    "# Ensure non-negative predictions\n",
    "y_test_pred = np.maximum(y_test_pred, 0)\n",
    "\n",
    "# Create a DataFrame for test predictions\n",
    "test_predictions = pd.DataFrame({\n",
    "    \"username\": test_usernames,\n",
    "    \"predicted_like_count\": y_test_pred\n",
    "})\n",
    "\n",
    "print(test_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      username  predicted_like_count\n",
      "0               beyazyakaliyiz           6558.287578\n",
      "1   totalenergies_istasyonlari             20.860497\n",
      "2                  konforyatak             22.142566\n",
      "3                     ht_kulup            188.599836\n",
      "4                    ajansspor            762.109647\n",
      "5         yusufelibelediyesi08            143.722016\n",
      "6                     4bros.tr             92.746365\n",
      "7              groundy.kadikoy            605.133060\n",
      "8                drtubagunebak            136.641764\n",
      "9                   nihatcan11            176.347690\n",
      "10               mustafaaakcay            282.351146\n",
      "11                  mtmofamily           7699.112320\n",
      "12                 bonitoperde            152.304595\n",
      "13     balkanlardangelenlezzet             33.321607\n",
      "14      pedagoganne__gulozturk            459.071655\n",
      "15                      iktm34            126.996710\n",
      "16                    yudumyag            361.955106\n",
      "17      bulentozdemir.edebiyat            427.033451\n",
      "18              imtolstoyevski           4332.982638\n",
      "19                 yorkkadikoy            150.337630\n",
      "20                  akkaalinda            146.563435\n",
      "21            mutfaktayusufvar            510.160481\n",
      "22                   antmodern             41.366752\n",
      "23                turkuazkablo             58.345499\n",
      "24             immergasturkiye             37.279544\n",
      "25              vanillaantalya            238.744082\n",
      "26                  ermanyasar           5557.363632\n",
      "27                   tuna.food            189.880965\n",
      "28        rustik.rus.restorani             47.532832\n",
      "29                    ilhansen           5758.738755\n",
      "30   institutfrancaisdeturquie            188.178761\n",
      "31      beachandbeyondswimwear            113.992045\n",
      "32             monsternotebook            326.477968\n",
      "33                     bsynctr             32.537553\n",
      "34                     kbbzone             17.419604\n",
      "35                     dbbanyo             27.206917\n",
      "36          istanbulalpplastik            214.483466\n",
      "37        enka_insaat_official             52.764853\n",
      "38              ozyufka.com_tr             97.209420\n",
      "39                nationalturk            244.985583\n",
      "40             sametkaankuyucu           7489.970568\n",
      "41               tutbelediyesi             76.365868\n",
      "42                      odurla            160.312753\n",
      "43                   balagency             55.047901\n",
      "44              sokebelediyesi            167.877540\n",
      "45   dedemanpalandokenskilodge             50.177517\n",
      "46               binefismutfak            542.263752\n",
      "47                tatli.sanati            748.790935\n",
      "48            evimin.yemekleri            403.374282\n",
      "49                ceyo_turkiye             13.789352\n"
     ]
    }
   ],
   "source": [
    "print(test_predictions.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.isnan(y_val).any() or np.isnan(y_pred_val).any():\n",
    "    print(\"NaN values found in y_val or y_pred_val!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Log Errors: 727.0928358570148\n"
     ]
    }
   ],
   "source": [
    "log_errors = np.log1p(y_val) - np.log1p(y_pred_val)  # Use np.log1p to handle zero values gracefully\n",
    "\n",
    "# Compute the absolute log errors\n",
    "abs_log_errors = np.abs(log_errors)\n",
    "\n",
    "# Sum of log errors\n",
    "sum_log_errors = np.sum(abs_log_errors)\n",
    "\n",
    "print(f\"Sum of Log Errors: {sum_log_errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log MSE Test= 2.975259638495896\n"
     ]
    }
   ],
   "source": [
    "print(f\"Log MSE Test= {log_mse_like_counts(y_val, y_pred_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[140], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV\n\u001b[0;32m     12\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mlgbm_regressor, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters found: \u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1015\u001b[0m     )\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1019\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:1573\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1573\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:965\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    960\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    961\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    962\u001b[0m         )\n\u001b[0;32m    963\u001b[0m     )\n\u001b[1;32m--> 965\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    988\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.05, 0.1, 0.15],\n",
    "    'max_depth': [5, 10, -1],\n",
    "    'min_data_in_leaf': [10, 20, 30],\n",
    "    'feature_fraction': [0.8, 0.9],\n",
    "    'bagging_fraction': [0.8, 0.9]\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(estimator=lgbm_regressor, param_grid=param_grid, cv=3, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation set: 2819676604.812782\n",
      "Sum of Log Errors: 1278.0898952402472\n",
      "                     username  predicted_like_count\n",
      "0              beyazyakaliyiz          10770.258789\n",
      "1  totalenergies_istasyonlari            453.646881\n",
      "2                 konforyatak            370.162567\n",
      "3                    ht_kulup            484.007294\n",
      "4                   ajansspor           2237.495117\n"
     ]
    }
   ],
   "source": [
    "'''from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare the target variable: Average like count per user\n",
    "user_avg_likes = {}\n",
    "for username, posts in username2posts_train.items():\n",
    "    avg_likes = np.mean([post.get(\"like_count\", 0) or 0 for post in posts])\n",
    "    user_avg_likes[username] = avg_likes\n",
    "\n",
    "# Create the target array\n",
    "y_like_counts = [user_avg_likes[uname] for uname in train_usernames]\n",
    "y_like_counts = [0 if np.isnan(val) else val for val in y_like_counts]\n",
    "# Train-test split\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_post_train, y_like_counts, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "xgb_regressor = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    objective='reg:squarederror',  # Default; can change to 'reg:poisson' for count data\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_regressor.fit(x_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred_val = xgb_regressor.predict(x_val)\n",
    "\n",
    "# Post-process: Clamp negative predictions to 0\n",
    "y_pred_val = np.maximum(y_pred_val, 0)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_val, y_pred_val)\n",
    "log_errors = np.log1p(y_val) - np.log1p(y_pred_val)  # Use np.log1p to handle zero values\n",
    "abs_log_errors = np.abs(log_errors)\n",
    "sum_log_errors = np.sum(abs_log_errors)\n",
    "\n",
    "print(f\"MSE on validation set: {mse}\")\n",
    "print(f\"Sum of Log Errors: {sum_log_errors}\")\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = xgb_regressor.predict(x_post_test)\n",
    "\n",
    "# Post-process: Clamp negative predictions to 0\n",
    "y_test_pred = np.maximum(y_test_pred, 0)\n",
    "\n",
    "# Create a DataFrame for test predictions\n",
    "test_predictions = pd.DataFrame({\n",
    "    \"username\": test_usernames,\n",
    "    \"predicted_like_count\": y_test_pred\n",
    "})\n",
    "\n",
    "print(test_predictions.head())\n",
    "\n",
    "# Save test predictions to a CSV file\n",
    "#test_predictions.to_csv(\"test_predictions.csv\", index=False)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN detected for user: touchdownistanbul, posts: []\n",
      "NaN detected for user: belediyesikose, posts: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for NaN in user_avg_likes\n",
    "for username, posts in username2posts_train.items():\n",
    "    avg_likes = np.mean([post.get(\"like_count\", 0) or 0 for post in posts])\n",
    "    if np.isnan(avg_likes):\n",
    "        print(f\"NaN detected for user: {username}, posts: {posts}\")\n",
    "    user_avg_likes[username] = avg_likes\n",
    "\n",
    "\n",
    "user_avg_likes[\"touchdownistanbul\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "Predictions saved to output_predictions_reg.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the test file\n",
    "test_data_path = \"/Users/Osama/Downloads/CS412PROJ/test-regression-round3.jsonl\"\n",
    "test_data = []\n",
    "with open(test_data_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        test_data.append(json.loads(line.strip()))\n",
    "\n",
    "\n",
    "\n",
    "# Extract user IDs and captions\n",
    "test_user_ids = [entry[\"id\"] for entry in test_data]\n",
    "test_unames = [entry[\"username\"] for entry in test_data]\n",
    "x_test = []\n",
    "\n",
    "for uname in test_unames:\n",
    "  try:\n",
    "    index = test_usernames.index(uname)\n",
    "    x_test.append(x_post_test[index].toarray()[0])\n",
    "  except Exception as e:\n",
    "    try:\n",
    "      index = train_usernames.index(uname)\n",
    "      x_test.append(x_post_train[index].toarray()[0])\n",
    "    except Exception as e:\n",
    "      print(uname)\n",
    "\n",
    "'''\n",
    "for id in test_user_ids:\n",
    "  try:\n",
    "    index = test_user_ids.index(id)\n",
    "    x_test.append(x_post_test_selected[index].toarray()[0])\n",
    "  except Exception as e:\n",
    "    try:\n",
    "      index = train_usernames.index(id)\n",
    "      x_test.append(x_post_train_selected[index].toarray()[0])\n",
    "    except Exception as e:\n",
    "      print(id)\n",
    "'''\n",
    "\n",
    "#test_captions = [entry.get(\"caption\", \"\") for entry in test_data]\n",
    "\n",
    "# Preprocess test captions\n",
    "#preprocessed_captions = [preprocess_text(caption) for caption in test_captions]\n",
    "\n",
    "# Transform test captions using trained vectorizer and selector\n",
    "#x_post_test_transformed = vectorizer.transform(preprocessed_captions)\n",
    "#x_post_test_selected = chi2_selector.transform(x_post_test_transformed)\n",
    "\n",
    "# Make predictions\n",
    "y_test_predictions = lgbm_regressor.predict(x_test)\n",
    "\n",
    "# Ensure non-negative predictions\n",
    "y_test_predictions = np.maximum(y_test_predictions, 0)\n",
    "# Prepare the output dictionary\n",
    "output = {user_id: int(prediction) for user_id, prediction in zip(test_user_ids, y_test_predictions.tolist())}\n",
    "\n",
    "#output = {user_id: int(prediction) for user_id, prediction in zip(test_user_ids, y_test_predictions)}\n",
    "\n",
    "# Save predictions to a JSON file\n",
    "output_file_path = \"output_predictions_reg.json\"\n",
    "with open(output_file_path, \"w\") as output_file:\n",
    "    json.dump(output, output_file, indent=4)\n",
    "\n",
    "print(f\"Predictions saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'caption': 'Ayvalık Belediyesi Kasım Ayı Olağan Meclis Toplantısı’nı Vural Sineması Nejat Uygur Sahnesi’nde gerçekleştirdik.\\n\\nAlınan kararların Ayvalık’ımıza hayırlı olmasını dilerim.', 'comments_count': 10, 'id': '18299464882193238', 'media_type': 'CAROUSEL_ALBUM', 'media_url': 'https://scontent-sof1-2.cdninstagram.com/v/t51.29350-15/398502640_367498438952512_6710374471736346193_n.jpg?_nc_cat=107&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=QFdBTP4J-X8AX8jZhY4&_nc_ht=scontent-sof1-2.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfDe8viPR0XWIPWwP_ravKlJ1b7kqr_pxMs0DT7IIf6DIQ&oe=65585C5E', 'timestamp': '2023-11-01 12:43:50', 'username': 'mesuterginofficial'}\n"
     ]
    }
   ],
   "source": [
    "print(test_data[0])  # Print the first entry to inspect its structure\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
