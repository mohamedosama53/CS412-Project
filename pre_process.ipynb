{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Preprocessing Function\n",
    "def preprocess_text(text: str):\n",
    "    text = text.casefold()  # Lowercase (casefold for Turkish-specific)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r'[^a-zçğıöşü0-9\\s#@]', '', text)  # Remove special characters\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespaces\n",
    "    return text\n",
    "\n",
    "# Train Word2Vec model\n",
    "def train_word2vec(corpus, vector_size=100, window=5, min_count=1):\n",
    "    sentences = [text.split() for text in corpus]  # Tokenize sentences for Word2Vec\n",
    "    model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count)\n",
    "    return model\n",
    "\n",
    "# Generate document vectors using TF-IDF weights and Word2Vec embeddings\n",
    "def get_document_vectors(corpus, tfidf_vectorizer, word2vec_model):\n",
    "    tfidf_matrix = tfidf_vectorizer.transform(corpus)\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    document_vectors = []\n",
    "\n",
    "    for doc_idx, doc in enumerate(corpus):\n",
    "        words = doc.split()\n",
    "        tfidf_weights = tfidf_matrix[doc_idx].toarray().flatten()\n",
    "        doc_vector = np.zeros(word2vec_model.vector_size)\n",
    "        total_weight = 0\n",
    "\n",
    "        for idx, word in enumerate(feature_names):\n",
    "            if word in word2vec_model.wv:  # Check if the word exists in Word2Vec\n",
    "                word_vector = word2vec_model.wv[word]\n",
    "                weight = tfidf_weights[idx]\n",
    "                doc_vector += weight * word_vector\n",
    "                total_weight += weight\n",
    "\n",
    "        if total_weight > 0:\n",
    "            doc_vector /= total_weight  # Normalize by the total weight\n",
    "        document_vectors.append(doc_vector)\n",
    "\n",
    "    return np.array(document_vectors)\n",
    "\n",
    "# Load and preprocess training data\n",
    "corpus = []\n",
    "train_usernames = []\n",
    "\n",
    "for username, posts in username2posts_train.items():\n",
    "    train_usernames.append(username)\n",
    "    cleaned_captions = []\n",
    "\n",
    "    for post in posts:\n",
    "        post_caption = post.get(\"caption\", \"\")\n",
    "        if post_caption is None:\n",
    "            continue\n",
    "        post_caption = preprocess_text(post_caption)\n",
    "        if post_caption != \"\":\n",
    "            cleaned_captions.append(post_caption)\n",
    "\n",
    "    user_post_captions = \"\\n\".join(cleaned_captions)\n",
    "    corpus.append(user_post_captions)\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=turkish_stopwords, max_features=15000, min_df=10, ngram_range=(1, 3))\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = train_word2vec(corpus)\n",
    "\n",
    "# Generate document vectors for training data\n",
    "x_post_train = get_document_vectors(corpus, vectorizer, word2vec_model)\n",
    "y_train = [username2_category.get(uname, \"NA\") for uname in train_usernames]\n",
    "\n",
    "# Preprocess test data\n",
    "test_corpus = []\n",
    "test_usernames = []\n",
    "\n",
    "for username, posts in username2posts_test.items():\n",
    "    test_usernames.append(username)\n",
    "    cleaned_captions = []\n",
    "\n",
    "    for post in posts:\n",
    "        post_caption = post.get(\"caption\", \"\")\n",
    "        if post_caption is None:\n",
    "            continue\n",
    "        post_caption = preprocess_text(post_caption)\n",
    "        if post_caption != \"\":\n",
    "            cleaned_captions.append(post_caption)\n",
    "\n",
    "    user_post_captions = \"\\n\".join(cleaned_captions)\n",
    "    test_corpus.append(user_post_captions)\n",
    "\n",
    "# Generate document vectors for test data\n",
    "x_post_test = get_document_vectors(test_corpus, vectorizer, word2vec_model)\n",
    "\n",
    "# Now, x_post_train and x_post_test can be used for training and testing your model\n",
    "print(\"Training data vectors shape:\", x_post_train.shape)\n",
    "print(\"Test data vectors shape:\", x_post_test.shape)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
