{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Osama\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "turkish_stopwords = stopwords.words('turkish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classification_df = pd.read_csv(\"/Users/Osama/Downloads/CS412PROJ/train-classification.csv\",)\n",
    "train_classification_df = train_classification_df.rename(columns={'Unnamed: 0': 'user_id', 'label': 'category'})\n",
    "\n",
    "# Unifying labels\n",
    "train_classification_df[\"category\"] = train_classification_df[\"category\"].apply(str.lower)\n",
    "username2_category = train_classification_df.set_index(\"user_id\").to_dict()[\"category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>art</th>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entertainment</th>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fashion</th>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>food</th>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gaming</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health and lifestyle</th>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mom and children</th>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sports</th>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tech</th>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>travel</th>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      user_id\n",
       "category                     \n",
       "art                       191\n",
       "entertainment             323\n",
       "fashion                   299\n",
       "food                      511\n",
       "gaming                     13\n",
       "health and lifestyle      503\n",
       "mom and children          149\n",
       "sports                    113\n",
       "tech                      346\n",
       "travel                    294"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_classification_df.groupby(\"category\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tech'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username2_category[\"kod8net\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_classification_df = pd.read_csv(\"/Users/Osama/Downloads/CS412PROJ/train-classification.csv\",)\\ntrain_classification_df = train_classification_df.rename(columns={\\'Unnamed: 0\\': \\'user_id\\', \\'label\\': \\'category\\'})\\n\\n# Unifying labels\\ntrain_classification_df[\"category\"] = train_classification_df[\"category\"].apply(str.lower)\\nusername2_category = train_classification_df.set_index(\"user_id\").to_dict()[\"category\"]\\n\\n\\n\\n\\n# Load the additional CSV file\\nadditional_data_path = \"/Users/Osama/Downloads/CS412PROJ/annotated_users_CS412-2753ef4cf74e.csv\"\\nadditional_data_df = pd.read_csv(additional_data_path)\\n\\n# Extract and rename the relevant columns\\nadditional_data_df = additional_data_df[[\\'Unnamed: 0\\', \\'influencerCategory\\']].rename(columns={\\n    \\'Unnamed: 0\\': \\'user_id\\', \\n    \\'influencerCategory\\': \\'category\\'\\n})\\n\\nadditional_data_df = additional_data_df.dropna()\\n\\n\\n\\n# Convert the \\'category\\' column to string type and apply .lower(), handling any NaN or unexpected values\\nadditional_data_df[\\'category\\'] = additional_data_df[\\'category\\'].astype(str).fillna(\\'\\').apply(str.lower)\\n\\n# Append the new data to the original train_classification_df\\ntrain_classification_df = pd.concat([train_classification_df, additional_data_df], ignore_index=True)\\n\\n# Update the username2_category dictionary with the new data\\nusername2_category.update(additional_data_df.set_index(\"user_id\").to_dict()[\"category\"])\\n\\n# Check the updated data by viewing the first few rows\\nprint(train_classification_df.head())\\n\\n# Re-check the category distribution\\ntrain_classification_df.groupby(\"category\").count()'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''train_classification_df = pd.read_csv(\"/Users/Osama/Downloads/CS412PROJ/train-classification.csv\",)\n",
    "train_classification_df = train_classification_df.rename(columns={'Unnamed: 0': 'user_id', 'label': 'category'})\n",
    "\n",
    "# Unifying labels\n",
    "train_classification_df[\"category\"] = train_classification_df[\"category\"].apply(str.lower)\n",
    "username2_category = train_classification_df.set_index(\"user_id\").to_dict()[\"category\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the additional CSV file\n",
    "additional_data_path = \"/Users/Osama/Downloads/CS412PROJ/annotated_users_CS412-2753ef4cf74e.csv\"\n",
    "additional_data_df = pd.read_csv(additional_data_path)\n",
    "\n",
    "# Extract and rename the relevant columns\n",
    "additional_data_df = additional_data_df[['Unnamed: 0', 'influencerCategory']].rename(columns={\n",
    "    'Unnamed: 0': 'user_id', \n",
    "    'influencerCategory': 'category'\n",
    "})\n",
    "\n",
    "additional_data_df = additional_data_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "# Convert the 'category' column to string type and apply .lower(), handling any NaN or unexpected values\n",
    "additional_data_df['category'] = additional_data_df['category'].astype(str).fillna('').apply(str.lower)\n",
    "\n",
    "# Append the new data to the original train_classification_df\n",
    "train_classification_df = pd.concat([train_classification_df, additional_data_df], ignore_index=True)\n",
    "\n",
    "# Update the username2_category dictionary with the new data\n",
    "username2_category.update(additional_data_df.set_index(\"user_id\").to_dict()[\"category\"])\n",
    "\n",
    "# Check the updated data by viewing the first few rows\n",
    "print(train_classification_df.head())\n",
    "\n",
    "# Re-check the category distribution\n",
    "train_classification_df.groupby(\"category\").count()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"/Users/Osama/Downloads/CS412PROJ/training-dataset.jsonl.gz\"\n",
    "\n",
    "username2posts_train = dict()\n",
    "username2profile_train = dict()\n",
    "\n",
    "username2posts_test = dict()\n",
    "username2profile_test = dict()\n",
    "\n",
    "\n",
    "with gzip.open(train_data_path, \"rt\") as fh:\n",
    "  for line in fh:\n",
    "    sample = json.loads(line)\n",
    "\n",
    "    profile = sample[\"profile\"]\n",
    "    username = profile[\"username\"]\n",
    "    if username in username2_category:\n",
    "      # train data info\n",
    "      username2posts_train[username] = sample[\"posts\"]\n",
    "      username2profile_train[username] = profile\n",
    "\n",
    "\n",
    "    else:\n",
    "      # it is test data info\n",
    "      username2posts_test[username] = sample[\"posts\"]\n",
    "      username2profile_test[username] = profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2741, 44)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_profile_df = pd.DataFrame(username2profile_train).T.reset_index(drop=True)\n",
    "test_profile_df = pd.DataFrame(username2profile_test).T.reset_index(drop=True)\n",
    "\n",
    "train_profile_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>id</th>\n",
       "      <th>full_name</th>\n",
       "      <th>biography</th>\n",
       "      <th>category_name</th>\n",
       "      <th>post_count</th>\n",
       "      <th>follower_count</th>\n",
       "      <th>following_count</th>\n",
       "      <th>is_business_account</th>\n",
       "      <th>is_private</th>\n",
       "      <th>...</th>\n",
       "      <th>business_category_name</th>\n",
       "      <th>overall_category_name</th>\n",
       "      <th>category_enum</th>\n",
       "      <th>is_verified_by_mv4b</th>\n",
       "      <th>is_regulated_c18</th>\n",
       "      <th>profile_pic_url</th>\n",
       "      <th>should_show_category</th>\n",
       "      <th>should_show_public_contacts</th>\n",
       "      <th>show_account_transparency_details</th>\n",
       "      <th>profile_picture_base64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beyazyakaliyiz</td>\n",
       "      <td>8634457436</td>\n",
       "      <td>Selam Beyaz Yakalı</td>\n",
       "      <td>Beyaz yakalıların dünyasına hoşgeldiniz 😀😀😀</td>\n",
       "      <td>Personal blog</td>\n",
       "      <td>None</td>\n",
       "      <td>1265</td>\n",
       "      <td>665</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>PERSONAL_BLOG</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://instagram.fist6-1.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>totalenergies_istasyonlari</td>\n",
       "      <td>7066643793</td>\n",
       "      <td>TotalEnergies İstasyonları</td>\n",
       "      <td>TotalEnergies İstasyonları resmi Instagram hes...</td>\n",
       "      <td>Energy Company</td>\n",
       "      <td>None</td>\n",
       "      <td>28025</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ENERGY_COMPANY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://instagram.fsaw2-1.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     username          id                    full_name  \\\n",
       "0              beyazyakaliyiz  8634457436           Selam Beyaz Yakalı   \n",
       "1  totalenergies_istasyonlari  7066643793  TotalEnergies İstasyonları   \n",
       "\n",
       "                                           biography   category_name  \\\n",
       "0        Beyaz yakalıların dünyasına hoşgeldiniz 😀😀😀   Personal blog   \n",
       "1  TotalEnergies İstasyonları resmi Instagram hes...  Energy Company   \n",
       "\n",
       "  post_count follower_count following_count is_business_account is_private  \\\n",
       "0       None           1265             665                True      False   \n",
       "1       None          28025               4                True      False   \n",
       "\n",
       "   ... business_category_name overall_category_name   category_enum  \\\n",
       "0  ...                   None                  None   PERSONAL_BLOG   \n",
       "1  ...                   None                  None  ENERGY_COMPANY   \n",
       "\n",
       "  is_verified_by_mv4b is_regulated_c18  \\\n",
       "0               False            False   \n",
       "1               False            False   \n",
       "\n",
       "                                     profile_pic_url should_show_category  \\\n",
       "0  https://instagram.fist6-1.fna.fbcdn.net/v/t51....                 True   \n",
       "1  https://instagram.fsaw2-1.fna.fbcdn.net/v/t51....                 True   \n",
       "\n",
       "  should_show_public_contacts show_account_transparency_details  \\\n",
       "0                        True                              True   \n",
       "1                        True                              True   \n",
       "\n",
       "                              profile_picture_base64  \n",
       "0  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
       "1  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
       "\n",
       "[2 rows x 44 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_profile_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bir            192.474290\n",
      "the            106.220712\n",
      "and             65.960927\n",
      "to              59.345598\n",
      "of              59.225437\n",
      "olsun           56.723444\n",
      "yeni            46.531255\n",
      "istanbul        46.083638\n",
      "in              44.908546\n",
      "olarak          43.520030\n",
      "olan            41.210234\n",
      "with            39.918309\n",
      "kutlu           39.892111\n",
      "kutlu olsun     39.315072\n",
      "iyi             37.609096\n",
      "güzel           37.207437\n",
      "kadar           36.265992\n",
      "you             35.986349\n",
      "for             34.862311\n",
      "özel            32.566009\n",
      "bilgi           32.489645\n",
      "devam           32.122644\n",
      "var             31.813758\n",
      "our             31.544601\n",
      "kasım           31.361084\n",
      "günü            31.188695\n",
      "birlikte        30.770312\n",
      "büyük           30.195825\n",
      "ilk             29.995598\n",
      "teşekkür        29.513940\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "\n",
    "def preprocess_text(text: str):\n",
    "    \n",
    "\n",
    "    # lower casing Turkish Text, Don't use str.lower :)\n",
    "    text = text.casefold()\n",
    "\n",
    "    #text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove special characters and punctuation\n",
    "    # HERE THE EMOJIS stuff are being removed, you may want to keep them :D\n",
    "    text = re.sub(r'[^a-zçğıöşü0-9\\s#@]', '', text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "corpus = []\n",
    "\n",
    "# to keep the label order\n",
    "train_usernames = []\n",
    "\n",
    "for username, posts in username2posts_train.items():\n",
    "  train_usernames.append(username)\n",
    "\n",
    "  # aggregating the posts per user\n",
    "  cleaned_captions = []\n",
    "  for post in posts:\n",
    "    post_caption = post.get(\"caption\", \"\")\n",
    "    if post_caption is None:\n",
    "      continue\n",
    "\n",
    "    post_caption = preprocess_text(post_caption)\n",
    "\n",
    "    if post_caption != \"\":\n",
    "      cleaned_captions.append(post_caption)\n",
    "\n",
    "\n",
    "  # joining the posts of each user with a \\n\n",
    "  user_post_captions = \"\\n\".join(cleaned_captions)\n",
    "  corpus.append(user_post_captions)\n",
    "\n",
    "\n",
    "\n",
    "#custom_stopwords = list(set(turkish_stopwords).union({\n",
    " #   'the', 'and', 'with', 'for', 'you', 'to', 'of', 'in', 'our', 'your', 'is', 'are','bir'\n",
    "#}))\n",
    "vectorizer = TfidfVectorizer(stop_words=turkish_stopwords, max_features=10000,min_df=10,ngram_range=(1, 3))\n",
    "\n",
    "# fit the vectorizer\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "# transform the data into vectors\n",
    "x_post_train = vectorizer.transform(corpus)\n",
    "y_train = [username2_category.get(uname, \"NA\") for uname in train_usernames]\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Inspect the frequency of each word\n",
    "df_tfidf = pd.DataFrame(x_post_train.toarray(), columns=feature_names)\n",
    "\n",
    "# Show the most frequent words (words in many posts)\n",
    "print(df_tfidf.sum().sort_values(ascending=False).head(30))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_usernames = []\n",
    "test_corpus = []\n",
    "for username, posts in username2posts_test.items():\n",
    "  test_usernames.append(username)\n",
    "  # aggregating the posts per user\n",
    "  cleaned_captions = []\n",
    "  for post in posts:\n",
    "    post_caption = post.get(\"caption\", \"\")\n",
    "    if post_caption is None:\n",
    "      continue\n",
    "\n",
    "    post_caption = preprocess_text(post_caption)\n",
    "\n",
    "    if post_caption != \"\":\n",
    "      cleaned_captions.append(post_caption)\n",
    "\n",
    "  user_post_captions = \"\\n\".join(cleaned_captions)\n",
    "  test_corpus.append(user_post_captions)\n",
    "\n",
    "\n",
    "# Just transforming! No Fitting!!!!!\n",
    "x_post_test = vectorizer.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure everything is fine\n",
    "assert y_train.count(\"NA\") == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ab', 'abant', 'abd', ..., 'şıklık', 'şıklığı', 'şımartın'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>abant</th>\n",
       "      <th>abd</th>\n",
       "      <th>abdulkadir</th>\n",
       "      <th>abdullah</th>\n",
       "      <th>abi</th>\n",
       "      <th>abiye</th>\n",
       "      <th>abone</th>\n",
       "      <th>about</th>\n",
       "      <th>about the</th>\n",
       "      <th>...</th>\n",
       "      <th>şöyle</th>\n",
       "      <th>şükran</th>\n",
       "      <th>şükranla</th>\n",
       "      <th>şükranlarımızı</th>\n",
       "      <th>şükür</th>\n",
       "      <th>şık</th>\n",
       "      <th>şık bir</th>\n",
       "      <th>şıklık</th>\n",
       "      <th>şıklığı</th>\n",
       "      <th>şımartın</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ab  abant  abd  abdulkadir  abdullah  abi  abiye  abone  about  about the  \\\n",
       "0  0.0    0.0  0.0         0.0       0.0  0.0    0.0    0.0    0.0        0.0   \n",
       "1  0.0    0.0  0.0         0.0       0.0  0.0    0.0    0.0    0.0        0.0   \n",
       "\n",
       "   ...  şöyle  şükran  şükranla  şükranlarımızı  şükür       şık  şık bir  \\\n",
       "0  ...    0.0     0.0       0.0             0.0    0.0  0.042156      0.0   \n",
       "1  ...    0.0     0.0       0.0             0.0    0.0  0.000000      0.0   \n",
       "\n",
       "   şıklık  şıklığı  şımartın  \n",
       "0     0.0      0.0       0.0  \n",
       "1     0.0      0.0       0.0  \n",
       "\n",
       "[2 rows x 10000 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf = pd.DataFrame(x_post_train.toarray(), columns=feature_names)\n",
    "df_tfidf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2741, 10000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df_tfidf: (2741, 10000)\n",
      "Length of y_train: 2741\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of df_tfidf:\", df_tfidf.shape)\n",
    "print(\"Length of y_train:\", len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(df_tfidf, y_train, test_size=0.2, stratify=y_train,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2192, 10000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(549, 10000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ozhotelstr\n",
      "elleturkiye\n",
      "sozerinsaatorhangazi\n",
      "sanliurfapiazzaavym\n",
      "rusanozden\n",
      "*****\n",
      "['ozhotelstr', 'elleturkiye', 'sozerinsaatorhangazi', 'sanliurfapiazzaavym', 'rusanozden']\n"
     ]
    }
   ],
   "source": [
    "#@title Test Data\n",
    "test_data_path = \"/Users/Osama/Downloads/CS412PROJ/test-classification-round1.dat\"\n",
    "\n",
    "with open(test_data_path, \"rt\") as fh:\n",
    "    for i, line in enumerate(fh):\n",
    "        print(line.strip())\n",
    "        if i >= 4:  # Stop after 5 lines\n",
    "            break\n",
    "\n",
    "print(\"*****\")\n",
    "\n",
    "test_unames = []\n",
    "with open(test_data_path, \"rt\") as fh:\n",
    "  for line in fh:\n",
    "    test_unames.append(line.strip())\n",
    "\n",
    "print(test_unames[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "screenname\n"
     ]
    }
   ],
   "source": [
    "x_test = []\n",
    "\n",
    "for uname in test_unames:\n",
    "  try:\n",
    "    index = test_usernames.index(uname)\n",
    "    x_test.append(x_post_test[index].toarray()[0])\n",
    "  except Exception as e:\n",
    "    try:\n",
    "      index = train_usernames.index(uname)\n",
    "      x_test.append(x_post_train[index].toarray()[0])\n",
    "    except Exception as e:\n",
    "      print(uname)\n",
    "\n",
    "\n",
    "test_unames.remove(\"screenname\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>abant</th>\n",
       "      <th>abd</th>\n",
       "      <th>abdulkadir</th>\n",
       "      <th>abdullah</th>\n",
       "      <th>abi</th>\n",
       "      <th>abiye</th>\n",
       "      <th>abone</th>\n",
       "      <th>about</th>\n",
       "      <th>about the</th>\n",
       "      <th>...</th>\n",
       "      <th>şöyle</th>\n",
       "      <th>şükran</th>\n",
       "      <th>şükranla</th>\n",
       "      <th>şükranlarımızı</th>\n",
       "      <th>şükür</th>\n",
       "      <th>şık</th>\n",
       "      <th>şık bir</th>\n",
       "      <th>şıklık</th>\n",
       "      <th>şıklığı</th>\n",
       "      <th>şımartın</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007865</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024666</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010521</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ab  abant       abd  abdulkadir  abdullah  abi  abiye  abone  about  \\\n",
       "0  0.0    0.0  0.000000         0.0       0.0  0.0    0.0    0.0    0.0   \n",
       "1  0.0    0.0  0.007865         0.0       0.0  0.0    0.0    0.0    0.0   \n",
       "\n",
       "   about the  ...     şöyle  şükran  şükranla  şükranlarımızı  şükür  \\\n",
       "0        0.0  ...  0.000000     0.0       0.0             0.0    0.0   \n",
       "1        0.0  ...  0.024666     0.0       0.0             0.0    0.0   \n",
       "\n",
       "        şık  şık bir  şıklık  şıklığı  şımartın  \n",
       "0  0.000000      0.0     0.0      0.0       0.0  \n",
       "1  0.010521      0.0     0.0      0.0       0.0  \n",
       "\n",
       "[2 rows x 10000 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame(np.array(x_test), columns=feature_names)\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_like_count(username, current_post=None):\n",
    "  def get_avg_like_count(posts:list):\n",
    "    total = 0.\n",
    "    for post in posts:\n",
    "      if current_post is not None and post[\"id\"] == current_post[\"id\"]:\n",
    "        continue\n",
    "\n",
    "      like_count = post.get(\"like_count\", 0)\n",
    "      if like_count is None:\n",
    "        like_count = 0\n",
    "      total += like_count\n",
    "\n",
    "    if len(posts) == 0:\n",
    "      return 0.\n",
    "\n",
    "    return total / len(posts)\n",
    "\n",
    "  if username in username2posts_train:\n",
    "    return get_avg_like_count(username2posts_train[username])\n",
    "  elif username in username2posts_test:\n",
    "    return get_avg_like_count(username2posts_test[username])\n",
    "  else:\n",
    "    print(f\"No data available for {username}\")\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_mse_like_counts(y_true, y_pred):\n",
    "  \"\"\"\n",
    "  Calculate the Log Mean Squared Error (Log MSE) for like counts (log(like_count + 1)).\n",
    "\n",
    "  Parameters:\n",
    "  - y_true: array-like, actual like counts\n",
    "  - y_pred: array-like, predicted like counts\n",
    "\n",
    "  Returns:\n",
    "  - log_mse: float, Log Mean Squared Error\n",
    "  \"\"\"\n",
    "  # Ensure inputs are numpy arrays\n",
    "  y_true = np.array(y_true)\n",
    "  y_pred = np.array(y_pred)\n",
    "\n",
    "  # Log transformation: log(like_count + 1)\n",
    "  log_y_true = np.log1p(y_true)\n",
    "  log_y_pred = np.log1p(y_pred)\n",
    "\n",
    "  # Compute squared errors\n",
    "  squared_errors = (log_y_true - log_y_pred) ** 2\n",
    "\n",
    "  # Return the mean of squared errors\n",
    "  return np.mean(squared_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log MSE Train= 1.2271047744059362\n"
     ]
    }
   ],
   "source": [
    "#@title Train Dataset evaluation\n",
    "\n",
    "y_like_count_train_true = []\n",
    "y_like_count_train_pred = []\n",
    "for uname, posts in username2posts_train.items():\n",
    "  for post in posts:\n",
    "    pred_val = predict_like_count(uname, post)\n",
    "    true_val = post.get(\"like_count\", 0)\n",
    "    if true_val is None:\n",
    "      true_val = 0\n",
    "\n",
    "    y_like_count_train_true.append(true_val)\n",
    "    y_like_count_train_pred.append(pred_val)\n",
    "\n",
    "print(f\"Log MSE Train= {log_mse_like_counts(y_like_count_train_true, y_like_count_train_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Test Dataset\n",
    "\n",
    "path = \"/Users/Osama/Downloads/CS412PROJ/test-regression-round1.jsonl\"\n",
    "output_path = \"/Users/Osama/Downloads/CS412PROJ/testy-regression-round1.jsonl\"\n",
    "\n",
    "to_predict_like_counts_usernames = []\n",
    "output_list = []\n",
    "with open(path, \"rt\") as fh:\n",
    "  for line in fh:\n",
    "    sample = json.loads(line)\n",
    "\n",
    "    # let's predict\n",
    "    pred_val = predict_like_count(sample[\"username\"])\n",
    "    sample[\"like_count\"] = int(pred_val)\n",
    "    output_list.append(sample)\n",
    "\n",
    "with open(output_path, \"wt\") as of:\n",
    "  json.dump(output_list, of)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'caption': 'KOZA 2023 2.si Damla’nın koleksiyonu, Latincede ‘Memento Mori’ '\n",
      "             'olarak bilinen ‘ölümlü olduğunu hatırla’ anlamındaki ifadeden '\n",
      "             'esinleniyor. Koleksiyon, hayatın ve ölümün, para, işçi, kral ve '\n",
      "             'kraliçe kavramları üzerinden yaratıcı görünümlerle bir araya '\n",
      "             'getirilmesini amaçlıyor. Ölüm sembollerinden esinlenen desenler '\n",
      "             'kullanan Damla, “kağıt parçasından ibaret olmak” kavramını '\n",
      "             'vurguluyor. Koleksiyon, yaşamın ve ölümün aynı anda ifade '\n",
      "             'edilmesini hedefliyor; kırmızı ve mavi ışıklarla veya '\n",
      "             'gözlüklerle görülen hologram efekti kullanılarak bu konsept '\n",
      "             'sahneye taşınıyor. Kırmızı renk ölümü, mavi ise yaşamı '\n",
      "             'simgeliyor. Koleksiyon, ofis giyimlerinden esinlenerek '\n",
      "             'kravatlar, gömlekler ve evrak çantaları içeriyor. Klasik sivri '\n",
      "             'burun çizmelerin üzerine spor ayakkabıların üst yüzeyi '\n",
      "             'yerleştirilerek, iş dünyasının koşuşturması ve cenaze '\n",
      "             'temalarının aynı anda ifade edilmesi amaçlanıyor. Para kazanma '\n",
      "             'arzusu, kırmızı zambak desenleri ve büyük mücevher görünümleri '\n",
      "             'ile koleksiyon tamamlanıyor.\\n'\n",
      "             '\\n'\n",
      "             'Tebrikler Damla!\\n'\n",
      "             '\\n'\n",
      "             '#GencModaTasarimcilari #Koza2023 #KozaYarismasi '\n",
      "             '#TasarimYarismasi #Moda #Fashion #ModaTasarımı',\n",
      "  'comments_count': 2,\n",
      "  'id': '18144550534306740',\n",
      "  'like_count': 158,\n",
      "  'media_type': 'CAROUSEL_ALBUM',\n",
      "  'media_url': 'https://scontent-sof1-1.cdninstagram.com/v/t51.29350-15/397997154_1016992459537522_4925783512176260397_n.jpg?_nc_cat=106&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=7V_eObkFeK4AX-LMtsK&_nc_ht=scontent-sof1-1.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfDEqDhzaTO3ezV-veT6cJFCOcAEyeVzHR6si9n33N6G5A&oe=6551B6B9',\n",
      "  'timestamp': '2023-11-02 15:49:22',\n",
      "  'username': 'kozayarismasi'},\n",
      " {'caption': 'Tüm Türkiye ve Avrupa’ya sevkiyatlarımız aralıksız devam ediyor! '\n",
      "             'Aracımız Bursa’dan Ordu’ya müşterimizin ürünleri için yola '\n",
      "             'çıkıyor.\\n'\n",
      "             '\\n'\n",
      "             '👉Tuna Mah. Etibank Cad. No:134 Osmangazi/BURSA\\n'\n",
      "             '\\n'\n",
      "             'www.celikbeymobilya.com sitemizden tüm modelleri '\n",
      "             'inceleyebilirsiniz. \\n'\n",
      "             '\\n'\n",
      "             '#bursa #almanya #fransa',\n",
      "  'comments_count': 0,\n",
      "  'id': '17995331788956693',\n",
      "  'like_count': 99,\n",
      "  'media_type': 'VIDEO',\n",
      "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m82/BF4767CB85BDFB8ADCCCA8F15B8C20B5_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmNsaXBzLnVua25vd24tQzMuNzIwLmRhc2hfYmFzZWxpbmVfMV92MSJ9&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=110&vs=1259525061418244_1441854817&_nc_vs=HBksFQIYT2lnX3hwdl9yZWVsc19wZXJtYW5lbnRfcHJvZC9CRjQ3NjdDQjg1QkRGQjhBRENDQ0E4RjE1QjhDMjBCNV92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dBRWdfaFZfcDVCYk5HZ0NBQTlzVURvZW5mZ3FicV9FQUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJvS3uOiQ0P8%2FFQIoAkMzLBdAJO%2Bdsi0OVhgSZGFzaF9iYXNlbGluZV8xX3YxEQB1AAA%3D&ccb=9-4&oh=00_AfAm22JssMPaUlQe3rpYsFWBhFb5mUgolTCdhV0Xgm4AnA&oe=6556A482&_nc_sid=1d576d&_nc_rid=cd9a998e44',\n",
      "  'timestamp': '2023-08-19 13:46:02',\n",
      "  'username': 'celikbeymobilya'},\n",
      " {'caption': '🤩\\n'\n",
      "             '\\n'\n",
      "             '.\\n'\n",
      "             '.\\n'\n",
      "             'Daha Fazlası İçin Beğenmeyi ve Takip Etmeyi Unutmayın\\n'\n",
      "             '.\\n'\n",
      "             'girisimci_muhendis ✅\\n'\n",
      "             '.\\n'\n",
      "             '📣Bu Bilgi Hakkında Ne Düşünüyorsunuz.\\n'\n",
      "             '.\\n'\n",
      "             '✅Görmesini İstediğin Arkadaşını Etiketle.\\n'\n",
      "             '.\\n'\n",
      "             '🔔Gönderi Bildirimlerini Açarak, Bilgileri Anında '\n",
      "             'Öğrenebilirsiniz.\\n'\n",
      "             '\\n'\n",
      "             '.\\n'\n",
      "             '\\n'\n",
      "             'Source: Unknown\\n'\n",
      "             'Dm for Credit or removal\\n'\n",
      "             '.\\n'\n",
      "             '.............................................................\\n'\n",
      "             'All rights and credits reserved to the respective owner(s). If '\n",
      "             'you are the main copyright owner rather than the one mentioned '\n",
      "             'here of this content, contact me to claim credit or content '\n",
      "             'removal',\n",
      "  'comments_count': 75,\n",
      "  'id': '18302703232191518',\n",
      "  'like_count': 1224,\n",
      "  'media_type': 'VIDEO',\n",
      "  'media_url': None,\n",
      "  'timestamp': '2023-10-02 06:53:33',\n",
      "  'username': 'girisimci_muhendis'}]\n"
     ]
    }
   ],
   "source": [
    "pprint(output_list[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "540 fits failed out of a total of 1620.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "173 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "367 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [            nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan -5.98151844e+08\n",
      " -6.00844463e+08 -5.96729551e+08 -5.95397727e+08 -5.90999870e+08\n",
      " -5.94984076e+08 -5.94943183e+08 -5.94648773e+08 -5.96943757e+08\n",
      " -5.88604049e+08 -5.94555434e+08 -5.96807916e+08 -5.89859485e+08\n",
      " -5.92729130e+08 -5.96283569e+08 -5.89109159e+08 -5.96113955e+08\n",
      " -5.99578221e+08 -5.98261731e+08 -5.95449607e+08 -5.96659047e+08\n",
      " -5.98261731e+08 -5.95449607e+08 -5.96659047e+08 -5.95391243e+08\n",
      " -5.95633163e+08 -5.98966226e+08 -6.16111817e+08 -6.04746670e+08\n",
      " -6.03428024e+08 -6.03317746e+08 -6.00727526e+08 -6.01422616e+08\n",
      " -6.06112695e+08 -6.04137039e+08 -6.06152163e+08 -6.15804016e+08\n",
      " -6.12744828e+08 -6.14041154e+08 -6.15041102e+08 -6.11851195e+08\n",
      " -6.14246604e+08 -6.19927124e+08 -6.15428908e+08 -6.14628899e+08\n",
      " -6.21038523e+08 -6.20993175e+08 -6.21872639e+08 -6.21038523e+08\n",
      " -6.20993175e+08 -6.21872639e+08 -6.22127889e+08 -6.21562889e+08\n",
      " -6.22245344e+08             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      " -6.16838128e+08 -6.13234512e+08 -6.16428246e+08 -6.20366012e+08\n",
      " -6.17451280e+08 -6.20343692e+08 -6.15809655e+08 -6.13284875e+08\n",
      " -6.17563369e+08 -6.08581718e+08 -6.13924864e+08 -6.17172945e+08\n",
      " -6.04727353e+08 -6.10359816e+08 -6.14575296e+08 -6.05839520e+08\n",
      " -6.10182594e+08 -6.15552930e+08 -6.10570062e+08 -6.08831020e+08\n",
      " -6.11685412e+08 -6.10570062e+08 -6.08831020e+08 -6.11685412e+08\n",
      " -6.09772780e+08 -6.08501457e+08 -6.12294856e+08 -6.46635886e+08\n",
      " -6.49024768e+08 -6.47215402e+08 -6.38781627e+08 -6.45246512e+08\n",
      " -6.47174627e+08 -6.43203313e+08 -6.46159828e+08 -6.46042889e+08\n",
      " -6.43091135e+08 -6.44960468e+08 -6.46738496e+08 -6.43299532e+08\n",
      " -6.44715502e+08 -6.45176408e+08 -6.43709307e+08 -6.45943273e+08\n",
      " -6.47100028e+08 -6.43109900e+08 -6.47090248e+08 -6.46855268e+08\n",
      " -6.43109900e+08 -6.47090248e+08 -6.46855268e+08 -6.42904323e+08\n",
      " -6.47011519e+08 -6.47528866e+08             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan -5.98552536e+08 -5.98237550e+08 -5.98357288e+08\n",
      " -6.00546513e+08 -6.00021969e+08 -6.03286163e+08 -6.05480894e+08\n",
      " -6.00667750e+08 -6.04192254e+08 -5.88790297e+08 -5.94609708e+08\n",
      " -6.00223727e+08 -5.91831440e+08 -5.97691216e+08 -6.00735204e+08\n",
      " -5.91401014e+08 -5.96786395e+08 -6.01635815e+08 -6.03350641e+08\n",
      " -6.01036096e+08 -6.02044182e+08 -6.03350641e+08 -6.01036096e+08\n",
      " -6.02044182e+08 -6.03731453e+08 -6.01113955e+08 -6.02468991e+08\n",
      " -6.15167845e+08 -6.16276234e+08 -6.16095674e+08 -6.22618193e+08\n",
      " -6.23267140e+08 -6.26082724e+08 -6.25351761e+08 -6.24542887e+08\n",
      " -6.25838420e+08 -6.25703167e+08 -6.23779692e+08 -6.28061103e+08\n",
      " -6.26382353e+08 -6.26291177e+08 -6.28915242e+08 -6.26549647e+08\n",
      " -6.26945107e+08 -6.29409497e+08 -6.33166376e+08 -6.32505241e+08\n",
      " -6.32105441e+08 -6.33166376e+08 -6.32505241e+08 -6.32105441e+08\n",
      " -6.31676419e+08 -6.31876440e+08 -6.32442643e+08             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan -6.03832157e+08 -5.99846599e+08\n",
      " -5.98971102e+08 -5.96099973e+08 -5.96430808e+08 -6.00020104e+08\n",
      " -5.96167970e+08 -5.96357468e+08 -5.98259798e+08 -5.85287138e+08\n",
      " -5.92927594e+08 -5.96281094e+08 -5.89127774e+08 -5.94468567e+08\n",
      " -5.97394930e+08 -5.93031765e+08 -5.98075944e+08 -6.01460130e+08\n",
      " -5.96485515e+08 -5.95865507e+08 -5.98646498e+08 -5.96485515e+08\n",
      " -5.95865507e+08 -5.98646498e+08 -5.99072339e+08 -5.96043524e+08\n",
      " -5.99669441e+08 -6.21689702e+08 -6.15375493e+08 -6.09603214e+08\n",
      " -6.15173286e+08 -6.15442889e+08 -6.15020198e+08 -6.17762547e+08\n",
      " -6.17682378e+08 -6.17007441e+08 -6.18245577e+08 -6.18478045e+08\n",
      " -6.21335940e+08 -6.20705659e+08 -6.19041105e+08 -6.21391870e+08\n",
      " -6.20461171e+08 -6.20614378e+08 -6.21775865e+08 -6.25981001e+08\n",
      " -6.24959844e+08 -6.25852792e+08 -6.25981001e+08 -6.24959844e+08\n",
      " -6.25852792e+08 -6.24860415e+08 -6.24524686e+08 -6.25850659e+08]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 30, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Initialize the regressor\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Best parameters and best model\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation set: 2819676604.812782\n",
      "                     username  predicted_like_count\n",
      "0              beyazyakaliyiz          10770.258789\n",
      "1  totalenergies_istasyonlari            453.646881\n",
      "2                 konforyatak            370.162567\n",
      "3                    ht_kulup            484.007294\n",
      "4                   ajansspor           2237.495117\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare the target variable: Average like count per user\n",
    "user_avg_likes = {}\n",
    "for username, posts in username2posts_train.items():\n",
    "    avg_likes = np.mean([post.get(\"like_count\", 0) or 0 for post in posts])\n",
    "    user_avg_likes[username] = avg_likes\n",
    "\n",
    "# Create the target array\n",
    "y_like_counts = [user_avg_likes[uname] for uname in train_usernames]\n",
    "y_like_counts = [0 if np.isnan(val) else val for val in y_like_counts]\n",
    "\n",
    "# Train-test split\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_post_train, y_like_counts, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train an MLP Regressor\n",
    "xgb_regressor = XGBRegressor(\n",
    "    n_estimators=100,   # Number of trees\n",
    "    learning_rate=0.1,  # Step size for updating weights\n",
    "    max_depth=6,        # Depth of each tree\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_regressor.fit(x_train, y_train)\n",
    "\n",
    "y_val = np.maximum(y_val, 0)\n",
    "\n",
    "\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred_val = xgb_regressor.predict(x_val)\n",
    "y_pred_val = np.maximum(y_pred_val, 0)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_val, y_pred_val)\n",
    "print(f\"MSE on validation set: {mse}\")\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = xgb_regressor.predict(x_post_test)\n",
    "\n",
    "\n",
    "# Create a DataFrame for test predictions\n",
    "test_predictions = pd.DataFrame({\n",
    "    \"username\": test_usernames,\n",
    "    \"predicted_like_count\": y_test_pred\n",
    "})\n",
    "print(test_predictions.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087490 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 210149\n",
      "[LightGBM] [Info] Number of data points in the train set: 2192, number of used features: 6824\n",
      "[LightGBM] [Info] Start training from score 8.538830\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "MSE on validation set: 3061670607.4357142\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "                     username  predicted_like_count\n",
      "0              beyazyakaliyiz           9599.277991\n",
      "1  totalenergies_istasyonlari             61.663299\n",
      "2                 konforyatak             24.042859\n",
      "3                    ht_kulup            169.114373\n",
      "4                   ajansspor            344.821231\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare the target variable: Average like count per user\n",
    "user_avg_likes = {}\n",
    "for username, posts in username2posts_train.items():\n",
    "    avg_likes = np.mean([post.get(\"like_count\", 0) or 0 for post in posts])\n",
    "    user_avg_likes[username] = avg_likes\n",
    "\n",
    "# Create the target array\n",
    "y_like_counts = [user_avg_likes[uname] for uname in train_usernames]\n",
    "y_like_counts = [0 if np.isnan(val) else val for val in y_like_counts]\n",
    "\n",
    "# Train-test split\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_post_train, y_like_counts, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "lgbm_regressor = LGBMRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.2,\n",
    "    max_depth=30,\n",
    "    min_data_in_leaf=30,\n",
    "    feature_fraction=0.8,\n",
    "    bagging_fraction=0.8,\n",
    "    objective=\"poisson\",  # Poisson objective for count data\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "lgbm_regressor.fit(x_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred_val = lgbm_regressor.predict(x_val)\n",
    "\n",
    "# Ensure non-negative predictions\n",
    "y_pred_val = np.maximum(y_pred_val, 0)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_val, y_pred_val)\n",
    "print(f\"MSE on validation set: {mse}\")\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = lgbm_regressor.predict(x_post_test)\n",
    "\n",
    "# Ensure non-negative predictions\n",
    "y_test_pred = np.maximum(y_test_pred, 0)\n",
    "\n",
    "# Create a DataFrame for test predictions\n",
    "test_predictions = pd.DataFrame({\n",
    "    \"username\": test_usernames,\n",
    "    \"predicted_like_count\": y_test_pred\n",
    "})\n",
    "\n",
    "print(test_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      username  predicted_like_count\n",
      "0               beyazyakaliyiz           6665.003543\n",
      "1   totalenergies_istasyonlari            118.704025\n",
      "2                  konforyatak             53.916728\n",
      "3                     ht_kulup            187.385827\n",
      "4                    ajansspor            333.354623\n",
      "5         yusufelibelediyesi08            159.517366\n",
      "6                     4bros.tr            200.266957\n",
      "7              groundy.kadikoy            586.239594\n",
      "8                drtubagunebak            214.565280\n",
      "9                   nihatcan11            284.355106\n",
      "10               mustafaaakcay            213.532438\n",
      "11                  mtmofamily          11993.537581\n",
      "12                 bonitoperde            131.077500\n",
      "13     balkanlardangelenlezzet             23.773820\n",
      "14      pedagoganne__gulozturk            577.488774\n",
      "15                      iktm34            278.959660\n",
      "16                    yudumyag            326.734851\n",
      "17      bulentozdemir.edebiyat            113.492775\n",
      "18              imtolstoyevski           2850.362776\n",
      "19                 yorkkadikoy            101.008715\n",
      "20                  akkaalinda            284.950170\n",
      "21            mutfaktayusufvar            719.695658\n",
      "22                   antmodern             24.044851\n",
      "23                turkuazkablo             78.400055\n",
      "24             immergasturkiye             21.097395\n",
      "25              vanillaantalya            341.771879\n",
      "26                  ermanyasar           7245.756170\n",
      "27                   tuna.food            440.740333\n",
      "28        rustik.rus.restorani             77.813804\n",
      "29                    ilhansen           1886.755950\n",
      "30   institutfrancaisdeturquie            188.975379\n",
      "31      beachandbeyondswimwear            192.366632\n",
      "32             monsternotebook            388.186551\n",
      "33                     bsynctr             34.359119\n",
      "34                     kbbzone             13.045094\n",
      "35                     dbbanyo             60.463167\n",
      "36          istanbulalpplastik            353.504641\n",
      "37        enka_insaat_official             85.179257\n",
      "38              ozyufka.com_tr            283.464199\n",
      "39                nationalturk            709.449589\n",
      "40             sametkaankuyucu           4208.685548\n",
      "41               tutbelediyesi             89.214539\n",
      "42                      odurla            319.611051\n",
      "43                   balagency             60.747986\n",
      "44              sokebelediyesi            107.300753\n",
      "45   dedemanpalandokenskilodge            209.398008\n",
      "46               binefismutfak           1583.649430\n",
      "47                tatli.sanati           2418.995707\n",
      "48            evimin.yemekleri            543.508957\n",
      "49                ceyo_turkiye             39.925230\n"
     ]
    }
   ],
   "source": [
    "print(test_predictions.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.isnan(y_val).any() or np.isnan(y_pred_val).any():\n",
    "    print(\"NaN values found in y_val or y_pred_val!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Log Errors: 728.0061700155054\n"
     ]
    }
   ],
   "source": [
    "log_errors = np.log1p(y_val) - np.log1p(y_pred_val)  # Use np.log1p to handle zero values gracefully\n",
    "\n",
    "# Compute the absolute log errors\n",
    "abs_log_errors = np.abs(log_errors)\n",
    "\n",
    "# Sum of log errors\n",
    "sum_log_errors = np.sum(abs_log_errors)\n",
    "\n",
    "print(f\"Sum of Log Errors: {sum_log_errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[140], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV\n\u001b[0;32m     12\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mlgbm_regressor, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters found: \u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1015\u001b[0m     )\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1019\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:1573\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1573\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:965\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    960\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    961\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    962\u001b[0m         )\n\u001b[0;32m    963\u001b[0m     )\n\u001b[1;32m--> 965\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    988\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.05, 0.1, 0.15],\n",
    "    'max_depth': [5, 10, -1],\n",
    "    'min_data_in_leaf': [10, 20, 30],\n",
    "    'feature_fraction': [0.8, 0.9],\n",
    "    'bagging_fraction': [0.8, 0.9]\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(estimator=lgbm_regressor, param_grid=param_grid, cv=3, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation set: 2819676604.812782\n",
      "Sum of Log Errors: 1278.0898952402472\n",
      "                     username  predicted_like_count\n",
      "0              beyazyakaliyiz          10770.258789\n",
      "1  totalenergies_istasyonlari            453.646881\n",
      "2                 konforyatak            370.162567\n",
      "3                    ht_kulup            484.007294\n",
      "4                   ajansspor           2237.495117\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare the target variable: Average like count per user\n",
    "user_avg_likes = {}\n",
    "for username, posts in username2posts_train.items():\n",
    "    avg_likes = np.mean([post.get(\"like_count\", 0) or 0 for post in posts])\n",
    "    user_avg_likes[username] = avg_likes\n",
    "\n",
    "# Create the target array\n",
    "y_like_counts = [user_avg_likes[uname] for uname in train_usernames]\n",
    "y_like_counts = [0 if np.isnan(val) else val for val in y_like_counts]\n",
    "# Train-test split\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_post_train, y_like_counts, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "xgb_regressor = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    objective='reg:squarederror',  # Default; can change to 'reg:poisson' for count data\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_regressor.fit(x_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred_val = xgb_regressor.predict(x_val)\n",
    "\n",
    "# Post-process: Clamp negative predictions to 0\n",
    "y_pred_val = np.maximum(y_pred_val, 0)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_val, y_pred_val)\n",
    "log_errors = np.log1p(y_val) - np.log1p(y_pred_val)  # Use np.log1p to handle zero values\n",
    "abs_log_errors = np.abs(log_errors)\n",
    "sum_log_errors = np.sum(abs_log_errors)\n",
    "\n",
    "print(f\"MSE on validation set: {mse}\")\n",
    "print(f\"Sum of Log Errors: {sum_log_errors}\")\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = xgb_regressor.predict(x_post_test)\n",
    "\n",
    "# Post-process: Clamp negative predictions to 0\n",
    "y_test_pred = np.maximum(y_test_pred, 0)\n",
    "\n",
    "# Create a DataFrame for test predictions\n",
    "test_predictions = pd.DataFrame({\n",
    "    \"username\": test_usernames,\n",
    "    \"predicted_like_count\": y_test_pred\n",
    "})\n",
    "\n",
    "print(test_predictions.head())\n",
    "\n",
    "# Save test predictions to a CSV file\n",
    "#test_predictions.to_csv(\"test_predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Log Errors: 1278.0898952402472\n"
     ]
    }
   ],
   "source": [
    "log_errors = np.log1p(y_val) - np.log1p(y_pred_val)  # Use np.log1p to handle zero values gracefully\n",
    "\n",
    "# Compute the absolute log errors\n",
    "abs_log_errors = np.abs(log_errors)\n",
    "\n",
    "# Sum of log errors\n",
    "sum_log_errors = np.sum(abs_log_errors)\n",
    "\n",
    "print(f\"Sum of Log Errors: {sum_log_errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'user_avg_likes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(avg_likes):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN detected for user: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00musername\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, posts: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mposts\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43muser_avg_likes\u001b[49m[username] \u001b[38;5;241m=\u001b[39m avg_likes\n",
      "\u001b[1;31mNameError\u001b[0m: name 'user_avg_likes' is not defined"
     ]
    }
   ],
   "source": [
    "# Check for NaN in user_avg_likes\n",
    "for username, posts in username2posts_train.items():\n",
    "    avg_likes = np.mean([post.get(\"like_count\", 0) or 0 for post in posts])\n",
    "    if np.isnan(avg_likes):\n",
    "        print(f\"NaN detected for user: {username}, posts: {posts}\")\n",
    "    user_avg_likes[username] = avg_likes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_val_pred_reg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate log errors\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m log_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(y_val_reg \u001b[38;5;241m-\u001b[39m \u001b[43my_val_pred_reg\u001b[49m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Sum of log errors\u001b[39;00m\n\u001b[0;32m      5\u001b[0m sum_log_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(log_errors)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_val_pred_reg' is not defined"
     ]
    }
   ],
   "source": [
    "'''# Calculate log errors\n",
    "log_errors = np.abs(y_val_reg - y_val_pred_reg)\n",
    "\n",
    "# Sum of log errors\n",
    "sum_log_errors = np.sum(log_errors)\n",
    "\n",
    "print(\"Sum of Log Errors:\", sum_log_errors)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
