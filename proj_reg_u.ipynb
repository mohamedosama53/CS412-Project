{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Osama\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "turkish_stopwords = stopwords.words('turkish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classification_df = pd.read_csv(\"/Users/Osama/Downloads/CS412PROJ/train-classification.csv\",)\n",
    "train_classification_df = train_classification_df.rename(columns={'Unnamed: 0': 'user_id', 'label': 'category'})\n",
    "\n",
    "# Unifying labels\n",
    "train_classification_df[\"category\"] = train_classification_df[\"category\"].apply(str.lower)\n",
    "username2_category = train_classification_df.set_index(\"user_id\").to_dict()[\"category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>art</th>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entertainment</th>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fashion</th>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>food</th>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gaming</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health and lifestyle</th>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mom and children</th>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sports</th>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tech</th>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>travel</th>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      user_id\n",
       "category                     \n",
       "art                       191\n",
       "entertainment             323\n",
       "fashion                   299\n",
       "food                      511\n",
       "gaming                     13\n",
       "health and lifestyle      503\n",
       "mom and children          149\n",
       "sports                    113\n",
       "tech                      346\n",
       "travel                    294"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_classification_df.groupby(\"category\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tech'"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username2_category[\"kod8net\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_classification_df = pd.read_csv(\"/Users/Osama/Downloads/CS412PROJ/train-classification.csv\",)\\ntrain_classification_df = train_classification_df.rename(columns={\\'Unnamed: 0\\': \\'user_id\\', \\'label\\': \\'category\\'})\\n\\n# Unifying labels\\ntrain_classification_df[\"category\"] = train_classification_df[\"category\"].apply(str.lower)\\nusername2_category = train_classification_df.set_index(\"user_id\").to_dict()[\"category\"]\\n\\n\\n\\n\\n# Load the additional CSV file\\nadditional_data_path = \"/Users/Osama/Downloads/CS412PROJ/annotated_users_CS412-2753ef4cf74e.csv\"\\nadditional_data_df = pd.read_csv(additional_data_path)\\n\\n# Extract and rename the relevant columns\\nadditional_data_df = additional_data_df[[\\'Unnamed: 0\\', \\'influencerCategory\\']].rename(columns={\\n    \\'Unnamed: 0\\': \\'user_id\\', \\n    \\'influencerCategory\\': \\'category\\'\\n})\\n\\nadditional_data_df = additional_data_df.dropna()\\n\\n\\n\\n# Convert the \\'category\\' column to string type and apply .lower(), handling any NaN or unexpected values\\nadditional_data_df[\\'category\\'] = additional_data_df[\\'category\\'].astype(str).fillna(\\'\\').apply(str.lower)\\n\\n# Append the new data to the original train_classification_df\\ntrain_classification_df = pd.concat([train_classification_df, additional_data_df], ignore_index=True)\\n\\n# Update the username2_category dictionary with the new data\\nusername2_category.update(additional_data_df.set_index(\"user_id\").to_dict()[\"category\"])\\n\\n# Check the updated data by viewing the first few rows\\nprint(train_classification_df.head())\\n\\n# Re-check the category distribution\\ntrain_classification_df.groupby(\"category\").count()'"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''train_classification_df = pd.read_csv(\"/Users/Osama/Downloads/CS412PROJ/train-classification.csv\",)\n",
    "train_classification_df = train_classification_df.rename(columns={'Unnamed: 0': 'user_id', 'label': 'category'})\n",
    "\n",
    "# Unifying labels\n",
    "train_classification_df[\"category\"] = train_classification_df[\"category\"].apply(str.lower)\n",
    "username2_category = train_classification_df.set_index(\"user_id\").to_dict()[\"category\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the additional CSV file\n",
    "additional_data_path = \"/Users/Osama/Downloads/CS412PROJ/annotated_users_CS412-2753ef4cf74e.csv\"\n",
    "additional_data_df = pd.read_csv(additional_data_path)\n",
    "\n",
    "# Extract and rename the relevant columns\n",
    "additional_data_df = additional_data_df[['Unnamed: 0', 'influencerCategory']].rename(columns={\n",
    "    'Unnamed: 0': 'user_id', \n",
    "    'influencerCategory': 'category'\n",
    "})\n",
    "\n",
    "additional_data_df = additional_data_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "# Convert the 'category' column to string type and apply .lower(), handling any NaN or unexpected values\n",
    "additional_data_df['category'] = additional_data_df['category'].astype(str).fillna('').apply(str.lower)\n",
    "\n",
    "# Append the new data to the original train_classification_df\n",
    "train_classification_df = pd.concat([train_classification_df, additional_data_df], ignore_index=True)\n",
    "\n",
    "# Update the username2_category dictionary with the new data\n",
    "username2_category.update(additional_data_df.set_index(\"user_id\").to_dict()[\"category\"])\n",
    "\n",
    "# Check the updated data by viewing the first few rows\n",
    "print(train_classification_df.head())\n",
    "\n",
    "# Re-check the category distribution\n",
    "train_classification_df.groupby(\"category\").count()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"/Users/Osama/Downloads/CS412PROJ/training-dataset.jsonl.gz\"\n",
    "\n",
    "username2posts_train = dict()\n",
    "username2profile_train = dict()\n",
    "\n",
    "username2posts_test = dict()\n",
    "username2profile_test = dict()\n",
    "\n",
    "\n",
    "with gzip.open(train_data_path, \"rt\") as fh:\n",
    "  for line in fh:\n",
    "    sample = json.loads(line)\n",
    "\n",
    "    profile = sample[\"profile\"]\n",
    "    username = profile[\"username\"]\n",
    "    if username in username2_category:\n",
    "      # train data info\n",
    "      username2posts_train[username] = sample[\"posts\"]\n",
    "      username2profile_train[username] = profile\n",
    "\n",
    "\n",
    "    else:\n",
    "      # it is test data info\n",
    "      username2posts_test[username] = sample[\"posts\"]\n",
    "      username2profile_test[username] = profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2741, 44)"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_profile_df = pd.DataFrame(username2profile_train).T.reset_index(drop=True)\n",
    "test_profile_df = pd.DataFrame(username2profile_test).T.reset_index(drop=True)\n",
    "\n",
    "train_profile_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>id</th>\n",
       "      <th>full_name</th>\n",
       "      <th>biography</th>\n",
       "      <th>category_name</th>\n",
       "      <th>post_count</th>\n",
       "      <th>follower_count</th>\n",
       "      <th>following_count</th>\n",
       "      <th>is_business_account</th>\n",
       "      <th>is_private</th>\n",
       "      <th>...</th>\n",
       "      <th>business_category_name</th>\n",
       "      <th>overall_category_name</th>\n",
       "      <th>category_enum</th>\n",
       "      <th>is_verified_by_mv4b</th>\n",
       "      <th>is_regulated_c18</th>\n",
       "      <th>profile_pic_url</th>\n",
       "      <th>should_show_category</th>\n",
       "      <th>should_show_public_contacts</th>\n",
       "      <th>show_account_transparency_details</th>\n",
       "      <th>profile_picture_base64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beyazyakaliyiz</td>\n",
       "      <td>8634457436</td>\n",
       "      <td>Selam Beyaz YakalÄ±</td>\n",
       "      <td>Beyaz yakalÄ±larÄ±n dÃ¼nyasÄ±na hoÅŸgeldiniz ðŸ˜€ðŸ˜€ðŸ˜€</td>\n",
       "      <td>Personal blog</td>\n",
       "      <td>None</td>\n",
       "      <td>1265</td>\n",
       "      <td>665</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>PERSONAL_BLOG</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://instagram.fist6-1.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>totalenergies_istasyonlari</td>\n",
       "      <td>7066643793</td>\n",
       "      <td>TotalEnergies IÌ‡stasyonlarÄ±</td>\n",
       "      <td>TotalEnergies Ä°stasyonlarÄ± resmi Instagram hes...</td>\n",
       "      <td>Energy Company</td>\n",
       "      <td>None</td>\n",
       "      <td>28025</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ENERGY_COMPANY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://instagram.fsaw2-1.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     username          id                    full_name  \\\n",
       "0              beyazyakaliyiz  8634457436           Selam Beyaz YakalÄ±   \n",
       "1  totalenergies_istasyonlari  7066643793  TotalEnergies IÌ‡stasyonlarÄ±   \n",
       "\n",
       "                                           biography   category_name  \\\n",
       "0        Beyaz yakalÄ±larÄ±n dÃ¼nyasÄ±na hoÅŸgeldiniz ðŸ˜€ðŸ˜€ðŸ˜€   Personal blog   \n",
       "1  TotalEnergies Ä°stasyonlarÄ± resmi Instagram hes...  Energy Company   \n",
       "\n",
       "  post_count follower_count following_count is_business_account is_private  \\\n",
       "0       None           1265             665                True      False   \n",
       "1       None          28025               4                True      False   \n",
       "\n",
       "   ... business_category_name overall_category_name   category_enum  \\\n",
       "0  ...                   None                  None   PERSONAL_BLOG   \n",
       "1  ...                   None                  None  ENERGY_COMPANY   \n",
       "\n",
       "  is_verified_by_mv4b is_regulated_c18  \\\n",
       "0               False            False   \n",
       "1               False            False   \n",
       "\n",
       "                                     profile_pic_url should_show_category  \\\n",
       "0  https://instagram.fist6-1.fna.fbcdn.net/v/t51....                 True   \n",
       "1  https://instagram.fsaw2-1.fna.fbcdn.net/v/t51....                 True   \n",
       "\n",
       "  should_show_public_contacts show_account_transparency_details  \\\n",
       "0                        True                              True   \n",
       "1                        True                              True   \n",
       "\n",
       "                              profile_picture_base64  \n",
       "0  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
       "1  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
       "\n",
       "[2 rows x 44 columns]"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_profile_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bir            192.474290\n",
      "the            106.220712\n",
      "and             65.960927\n",
      "to              59.345598\n",
      "of              59.225437\n",
      "olsun           56.723444\n",
      "yeni            46.531255\n",
      "istanbul        46.083638\n",
      "in              44.908546\n",
      "olarak          43.520030\n",
      "olan            41.210234\n",
      "with            39.918309\n",
      "kutlu           39.892111\n",
      "kutlu olsun     39.315072\n",
      "iyi             37.609096\n",
      "gÃ¼zel           37.207437\n",
      "kadar           36.265992\n",
      "you             35.986349\n",
      "for             34.862311\n",
      "Ã¶zel            32.566009\n",
      "bilgi           32.489645\n",
      "devam           32.122644\n",
      "var             31.813758\n",
      "our             31.544601\n",
      "kasÄ±m           31.361084\n",
      "gÃ¼nÃ¼            31.188695\n",
      "birlikte        30.770312\n",
      "bÃ¼yÃ¼k           30.195825\n",
      "ilk             29.995598\n",
      "teÅŸekkÃ¼r        29.513940\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "\n",
    "def preprocess_text(text: str):\n",
    "    \n",
    "\n",
    "    # lower casing Turkish Text, Don't use str.lower :)\n",
    "    text = text.casefold()\n",
    "\n",
    "    #text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove special characters and punctuation\n",
    "    # HERE THE EMOJIS stuff are being removed, you may want to keep them :D\n",
    "    text = re.sub(r'[^a-zÃ§ÄŸÄ±Ã¶ÅŸÃ¼0-9\\s#@]', '', text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "corpus = []\n",
    "\n",
    "# to keep the label order\n",
    "train_usernames = []\n",
    "\n",
    "for username, posts in username2posts_train.items():\n",
    "  train_usernames.append(username)\n",
    "\n",
    "  # aggregating the posts per user\n",
    "  cleaned_captions = []\n",
    "  for post in posts:\n",
    "    post_caption = post.get(\"caption\", \"\")\n",
    "    if post_caption is None:\n",
    "      continue\n",
    "\n",
    "    post_caption = preprocess_text(post_caption)\n",
    "\n",
    "    if post_caption != \"\":\n",
    "      cleaned_captions.append(post_caption)\n",
    "\n",
    "\n",
    "  # joining the posts of each user with a \\n\n",
    "  user_post_captions = \"\\n\".join(cleaned_captions)\n",
    "  corpus.append(user_post_captions)\n",
    "\n",
    "\n",
    "\n",
    "#custom_stopwords = list(set(turkish_stopwords).union({\n",
    " #   'the', 'and', 'with', 'for', 'you', 'to', 'of', 'in', 'our', 'your', 'is', 'are','bir'\n",
    "#}))\n",
    "vectorizer = TfidfVectorizer(stop_words=turkish_stopwords, max_features=10000,min_df=10,ngram_range=(1, 3))\n",
    "\n",
    "# fit the vectorizer\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "# transform the data into vectors\n",
    "x_post_train = vectorizer.transform(corpus)\n",
    "y_train = [username2_category.get(uname, \"NA\") for uname in train_usernames]\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Inspect the frequency of each word\n",
    "df_tfidf = pd.DataFrame(x_post_train.toarray(), columns=feature_names)\n",
    "\n",
    "# Show the most frequent words (words in many posts)\n",
    "print(df_tfidf.sum().sort_values(ascending=False).head(30))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_usernames = []\n",
    "test_corpus = []\n",
    "for username, posts in username2posts_test.items():\n",
    "  test_usernames.append(username)\n",
    "  # aggregating the posts per user\n",
    "  cleaned_captions = []\n",
    "  for post in posts:\n",
    "    post_caption = post.get(\"caption\", \"\")\n",
    "    if post_caption is None:\n",
    "      continue\n",
    "\n",
    "    post_caption = preprocess_text(post_caption)\n",
    "\n",
    "    if post_caption != \"\":\n",
    "      cleaned_captions.append(post_caption)\n",
    "\n",
    "  user_post_captions = \"\\n\".join(cleaned_captions)\n",
    "  test_corpus.append(user_post_captions)\n",
    "\n",
    "\n",
    "# Just transforming! No Fitting!!!!!\n",
    "x_post_test = vectorizer.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure everything is fine\n",
    "assert y_train.count(\"NA\") == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ab', 'abant', 'abd', ..., 'ÅŸÄ±klÄ±k', 'ÅŸÄ±klÄ±ÄŸÄ±', 'ÅŸÄ±martÄ±n'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>abant</th>\n",
       "      <th>abd</th>\n",
       "      <th>abdulkadir</th>\n",
       "      <th>abdullah</th>\n",
       "      <th>abi</th>\n",
       "      <th>abiye</th>\n",
       "      <th>abone</th>\n",
       "      <th>about</th>\n",
       "      <th>about the</th>\n",
       "      <th>...</th>\n",
       "      <th>ÅŸÃ¶yle</th>\n",
       "      <th>ÅŸÃ¼kran</th>\n",
       "      <th>ÅŸÃ¼kranla</th>\n",
       "      <th>ÅŸÃ¼kranlarÄ±mÄ±zÄ±</th>\n",
       "      <th>ÅŸÃ¼kÃ¼r</th>\n",
       "      <th>ÅŸÄ±k</th>\n",
       "      <th>ÅŸÄ±k bir</th>\n",
       "      <th>ÅŸÄ±klÄ±k</th>\n",
       "      <th>ÅŸÄ±klÄ±ÄŸÄ±</th>\n",
       "      <th>ÅŸÄ±martÄ±n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ab  abant  abd  abdulkadir  abdullah  abi  abiye  abone  about  about the  \\\n",
       "0  0.0    0.0  0.0         0.0       0.0  0.0    0.0    0.0    0.0        0.0   \n",
       "1  0.0    0.0  0.0         0.0       0.0  0.0    0.0    0.0    0.0        0.0   \n",
       "\n",
       "   ...  ÅŸÃ¶yle  ÅŸÃ¼kran  ÅŸÃ¼kranla  ÅŸÃ¼kranlarÄ±mÄ±zÄ±  ÅŸÃ¼kÃ¼r       ÅŸÄ±k  ÅŸÄ±k bir  \\\n",
       "0  ...    0.0     0.0       0.0             0.0    0.0  0.042156      0.0   \n",
       "1  ...    0.0     0.0       0.0             0.0    0.0  0.000000      0.0   \n",
       "\n",
       "   ÅŸÄ±klÄ±k  ÅŸÄ±klÄ±ÄŸÄ±  ÅŸÄ±martÄ±n  \n",
       "0     0.0      0.0       0.0  \n",
       "1     0.0      0.0       0.0  \n",
       "\n",
       "[2 rows x 10000 columns]"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf = pd.DataFrame(x_post_train.toarray(), columns=feature_names)\n",
    "df_tfidf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2741, 10000)"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df_tfidf: (2741, 10000)\n",
      "Length of y_train: 2741\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of df_tfidf:\", df_tfidf.shape)\n",
    "print(\"Length of y_train:\", len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from datetime import datetime\n",
    "def extract_features(posts):\n",
    "    features = []\n",
    "    for post in posts:\n",
    "        caption = post.get(\"caption\", \"\")\n",
    "        comments_count = post.get(\"comments_count\", 0)\n",
    "        like_count = post.get(\"like_count\", 0)\n",
    "        media_type = post.get(\"media_type\", \"UNKNOWN\")\n",
    "        timestamp = post.get(\"timestamp\", \"\")\n",
    "        \n",
    "        # Process timestamp\n",
    "        if timestamp:\n",
    "            dt = datetime.strptime(timestamp, \"%Y-%m-%d %H:%M:%S\")\n",
    "            hour = dt.hour\n",
    "            day = dt.day\n",
    "            month = dt.month\n",
    "        else:\n",
    "            hour = day = month = 0\n",
    "        \n",
    "        # Feature list\n",
    "        features.append({\n",
    "            \"caption\": caption,\n",
    "            \"comments_count\": comments_count,\n",
    "            \"media_type\": media_type,\n",
    "            \"hour\": hour,\n",
    "            \"day\": day,\n",
    "            \"month\": month,\n",
    "            \"like_count\": like_count  # Target variable\n",
    "        })\n",
    "    return features\n",
    "\n",
    "\n",
    "train_features = []\n",
    "train_targets = []\n",
    "\n",
    "for username, posts in username2posts_train.items():\n",
    "    user_features = extract_features(posts)\n",
    "    for feature in user_features:\n",
    "        train_features.append(feature)\n",
    "        train_targets.append(feature[\"like_count\"])\n",
    "\n",
    "# Create a DataFrame for easy manipulation\n",
    "train_df = pd.DataFrame(train_features)\n",
    "\n",
    "# Step 3: Feature Transformation\n",
    "# One-hot encode `media_type`\n",
    "vectorizer = TfidfVectorizer(max_features=10000, stop_words=turkish_stopwords,min_df=10,ngram_range=(1, 3))\n",
    "\n",
    "onehot_encoder = OneHotEncoder()\n",
    "media_type_encoded = onehot_encoder.fit_transform(train_df[[\"media_type\"]])\n",
    "\n",
    "# Apply TF-IDF vectorization to captions\n",
    "train_df[\"caption\"] = train_df[\"caption\"].fillna(\"\")\n",
    "\n",
    "# Apply TF-IDF vectorization to captions\n",
    "caption_tfidf = vectorizer.fit_transform(train_df[\"caption\"])\n",
    "\n",
    "# Combine features into a single matrix\n",
    "from scipy.sparse import hstack\n",
    "final_features = hstack([\n",
    "    caption_tfidf, \n",
    "    media_type_encoded, \n",
    "    train_df[[\"comments_count\", \"hour\", \"day\", \"month\"]].values\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [94824, 2192]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[421], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 3\u001b[0m x_train, x_val, y_train, y_val \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:2782\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2780\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2782\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2784\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   2785\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2786\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2787\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:514\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \n\u001b[0;32m    486\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;124;03m[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    513\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 514\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [94824, 2192]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(final_features, y_train, test_size=0.2, stratify=y_train,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2192, 10000)"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(549, 10000)"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ozhotelstr\n",
      "elleturkiye\n",
      "sozerinsaatorhangazi\n",
      "sanliurfapiazzaavym\n",
      "rusanozden\n",
      "*****\n",
      "['ozhotelstr', 'elleturkiye', 'sozerinsaatorhangazi', 'sanliurfapiazzaavym', 'rusanozden']\n"
     ]
    }
   ],
   "source": [
    "#@title Test Data\n",
    "test_data_path = \"/Users/Osama/Downloads/CS412PROJ/test-classification-round1.dat\"\n",
    "\n",
    "with open(test_data_path, \"rt\") as fh:\n",
    "    for i, line in enumerate(fh):\n",
    "        print(line.strip())\n",
    "        if i >= 4:  # Stop after 5 lines\n",
    "            break\n",
    "\n",
    "print(\"*****\")\n",
    "\n",
    "test_unames = []\n",
    "with open(test_data_path, \"rt\") as fh:\n",
    "  for line in fh:\n",
    "    test_unames.append(line.strip())\n",
    "\n",
    "print(test_unames[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "screenname\n"
     ]
    }
   ],
   "source": [
    "x_test = []\n",
    "\n",
    "for uname in test_unames:\n",
    "  try:\n",
    "    index = test_usernames.index(uname)\n",
    "    x_test.append(x_post_test[index].toarray()[0])\n",
    "  except Exception as e:\n",
    "    try:\n",
    "      index = train_usernames.index(uname)\n",
    "      x_test.append(x_post_train[index].toarray()[0])\n",
    "    except Exception as e:\n",
    "      print(uname)\n",
    "\n",
    "\n",
    "test_unames.remove(\"screenname\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>abant</th>\n",
       "      <th>abd</th>\n",
       "      <th>abdulkadir</th>\n",
       "      <th>abdullah</th>\n",
       "      <th>abi</th>\n",
       "      <th>abiye</th>\n",
       "      <th>abone</th>\n",
       "      <th>about</th>\n",
       "      <th>about the</th>\n",
       "      <th>...</th>\n",
       "      <th>ÅŸÃ¶yle</th>\n",
       "      <th>ÅŸÃ¼kran</th>\n",
       "      <th>ÅŸÃ¼kranla</th>\n",
       "      <th>ÅŸÃ¼kranlarÄ±mÄ±zÄ±</th>\n",
       "      <th>ÅŸÃ¼kÃ¼r</th>\n",
       "      <th>ÅŸÄ±k</th>\n",
       "      <th>ÅŸÄ±k bir</th>\n",
       "      <th>ÅŸÄ±klÄ±k</th>\n",
       "      <th>ÅŸÄ±klÄ±ÄŸÄ±</th>\n",
       "      <th>ÅŸÄ±martÄ±n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007865</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024666</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010521</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ab  abant       abd  abdulkadir  abdullah  abi  abiye  abone  about  \\\n",
       "0  0.0    0.0  0.000000         0.0       0.0  0.0    0.0    0.0    0.0   \n",
       "1  0.0    0.0  0.007865         0.0       0.0  0.0    0.0    0.0    0.0   \n",
       "\n",
       "   about the  ...     ÅŸÃ¶yle  ÅŸÃ¼kran  ÅŸÃ¼kranla  ÅŸÃ¼kranlarÄ±mÄ±zÄ±  ÅŸÃ¼kÃ¼r  \\\n",
       "0        0.0  ...  0.000000     0.0       0.0             0.0    0.0   \n",
       "1        0.0  ...  0.024666     0.0       0.0             0.0    0.0   \n",
       "\n",
       "        ÅŸÄ±k  ÅŸÄ±k bir  ÅŸÄ±klÄ±k  ÅŸÄ±klÄ±ÄŸÄ±  ÅŸÄ±martÄ±n  \n",
       "0  0.000000      0.0     0.0      0.0       0.0  \n",
       "1  0.010521      0.0     0.0      0.0       0.0  \n",
       "\n",
       "[2 rows x 10000 columns]"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame(np.array(x_test), columns=feature_names)\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_features(posts):\n",
    "    features = []\n",
    "    for post in posts:\n",
    "        caption = post.get(\"caption\", \"\")\n",
    "        comments_count = post.get(\"comments_count\", 0)\n",
    "        like_count = post.get(\"like_count\", 0)\n",
    "        media_type = post.get(\"media_type\", \"UNKNOWN\")\n",
    "        timestamp = post.get(\"timestamp\", \"\")\n",
    "        \n",
    "        # Process timestamp\n",
    "        if timestamp:\n",
    "            dt = datetime.strptime(timestamp, \"%Y-%m-%d %H:%M:%S\")\n",
    "            hour = dt.hour\n",
    "            day = dt.day\n",
    "            month = dt.month\n",
    "        else:\n",
    "            hour = day = month = 0\n",
    "        \n",
    "        # Feature list\n",
    "        features.append({\n",
    "            \"caption\": caption,\n",
    "            \"comments_count\": comments_count,\n",
    "            \"media_type\": media_type,\n",
    "            \"hour\": hour,\n",
    "            \"day\": day,\n",
    "            \"month\": month,\n",
    "            \"like_count\": like_count  # Target variable\n",
    "        })\n",
    "    return feature\n",
    "\n",
    "train_features = []\n",
    "train_targets = []\n",
    "for username, posts in username2posts_train.items():\n",
    "    user_features = extract_features(posts)\n",
    "    for feature in user_features:\n",
    "        train_features.append(feature)\n",
    "        train_targets.append(feature[\"like_count\"])\n",
    "\n",
    "# Create DataFrame\n",
    "train_df = pd.DataFrame(train_features)\n",
    "\n",
    "# Encode `media_type` as one-hot\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "media_type_encoded = onehot_encoder.fit_transform(train_df[[\"media_type\"]])\n",
    "\n",
    "# TF-IDF on captions\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words=turkish_stopwords)\n",
    "caption_tfidf = vectorizer.fit_transform(train_df[\"caption\"])\n",
    "\n",
    "# Combine features\n",
    "from scipy.sparse import hstack\n",
    "final_features = hstack([\n",
    "    caption_tfidf, \n",
    "    media_type_encoded, \n",
    "    train_df[[\"comments_count\", \"hour\", \"day\", \"month\"]].values\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_like_count(username, current_post=None):\n",
    "  def get_avg_like_count(posts:list):\n",
    "    total = 0.\n",
    "    for post in posts:\n",
    "      if current_post is not None and post[\"id\"] == current_post[\"id\"]:\n",
    "        continue\n",
    "\n",
    "      like_count = post.get(\"like_count\", 0)\n",
    "      if like_count is None:\n",
    "        like_count = 0\n",
    "      total += like_count\n",
    "\n",
    "    if len(posts) == 0:\n",
    "      return 0.\n",
    "\n",
    "    return total / len(posts)\n",
    "\n",
    "  if username in username2posts_train:\n",
    "    return get_avg_like_count(username2posts_train[username])\n",
    "  elif username in username2posts_test:\n",
    "    return get_avg_like_count(username2posts_test[username])\n",
    "  else:\n",
    "    print(f\"No data available for {username}\")\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_mse_like_counts(y_true, y_pred):\n",
    "  \"\"\"\n",
    "  Calculate the Log Mean Squared Error (Log MSE) for like counts (log(like_count + 1)).\n",
    "\n",
    "  Parameters:\n",
    "  - y_true: array-like, actual like counts\n",
    "  - y_pred: array-like, predicted like counts\n",
    "\n",
    "  Returns:\n",
    "  - log_mse: float, Log Mean Squared Error\n",
    "  \"\"\"\n",
    "  # Ensure inputs are numpy arrays\n",
    "  y_true = np.array(y_true)\n",
    "  y_pred = np.array(y_pred)\n",
    "\n",
    "  # Log transformation: log(like_count + 1)\n",
    "  log_y_true = np.log1p(y_true)\n",
    "  log_y_pred = np.log1p(y_pred)\n",
    "\n",
    "  # Compute squared errors\n",
    "  squared_errors = (log_y_true - log_y_pred) ** 2\n",
    "\n",
    "  # Return the mean of squared errors\n",
    "  return np.mean(squared_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log MSE Train= 1.2271047744059362\n"
     ]
    }
   ],
   "source": [
    "#@title Train Dataset evaluation\n",
    "\n",
    "y_like_count_train_true = []\n",
    "y_like_count_train_pred = []\n",
    "for uname, posts in username2posts_train.items():\n",
    "  for post in posts:\n",
    "    pred_val = predict_like_count(uname, post)\n",
    "    true_val = post.get(\"like_count\", 0)\n",
    "    if true_val is None:\n",
    "      true_val = 0\n",
    "\n",
    "    y_like_count_train_true.append(true_val)\n",
    "    y_like_count_train_pred.append(pred_val)\n",
    "\n",
    "print(f\"Log MSE Train= {log_mse_like_counts(y_like_count_train_true, y_like_count_train_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Test Dataset\n",
    "\n",
    "path = \"/Users/Osama/Downloads/CS412PROJ/test-regression-round1.jsonl\"\n",
    "output_path = \"/Users/Osama/Downloads/CS412PROJ/testy-regression-round1.jsonl\"\n",
    "\n",
    "to_predict_like_counts_usernames = []\n",
    "output_list = []\n",
    "with open(path, \"rt\") as fh:\n",
    "  for line in fh:\n",
    "    sample = json.loads(line)\n",
    "\n",
    "    # let's predict\n",
    "    pred_val = predict_like_count(sample[\"username\"])\n",
    "    sample[\"like_count\"] = int(pred_val)\n",
    "    output_list.append(sample)\n",
    "\n",
    "with open(output_path, \"wt\") as of:\n",
    "  json.dump(output_list, of)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'caption': 'KOZA 2023 2.si Damlaâ€™nÄ±n koleksiyonu, Latincede â€˜Memento Moriâ€™ '\n",
      "             'olarak bilinen â€˜Ã¶lÃ¼mlÃ¼ olduÄŸunu hatÄ±rlaâ€™ anlamÄ±ndaki ifadeden '\n",
      "             'esinleniyor. Koleksiyon, hayatÄ±n ve Ã¶lÃ¼mÃ¼n, para, iÅŸÃ§i, kral ve '\n",
      "             'kraliÃ§e kavramlarÄ± Ã¼zerinden yaratÄ±cÄ± gÃ¶rÃ¼nÃ¼mlerle bir araya '\n",
      "             'getirilmesini amaÃ§lÄ±yor. Ã–lÃ¼m sembollerinden esinlenen desenler '\n",
      "             'kullanan Damla, â€œkaÄŸÄ±t parÃ§asÄ±ndan ibaret olmakâ€ kavramÄ±nÄ± '\n",
      "             'vurguluyor. Koleksiyon, yaÅŸamÄ±n ve Ã¶lÃ¼mÃ¼n aynÄ± anda ifade '\n",
      "             'edilmesini hedefliyor; kÄ±rmÄ±zÄ± ve mavi Ä±ÅŸÄ±klarla veya '\n",
      "             'gÃ¶zlÃ¼klerle gÃ¶rÃ¼len hologram efekti kullanÄ±larak bu konsept '\n",
      "             'sahneye taÅŸÄ±nÄ±yor. KÄ±rmÄ±zÄ± renk Ã¶lÃ¼mÃ¼, mavi ise yaÅŸamÄ± '\n",
      "             'simgeliyor. Koleksiyon, ofis giyimlerinden esinlenerek '\n",
      "             'kravatlar, gÃ¶mlekler ve evrak Ã§antalarÄ± iÃ§eriyor. Klasik sivri '\n",
      "             'burun Ã§izmelerin Ã¼zerine spor ayakkabÄ±larÄ±n Ã¼st yÃ¼zeyi '\n",
      "             'yerleÅŸtirilerek, iÅŸ dÃ¼nyasÄ±nÄ±n koÅŸuÅŸturmasÄ± ve cenaze '\n",
      "             'temalarÄ±nÄ±n aynÄ± anda ifade edilmesi amaÃ§lanÄ±yor. Para kazanma '\n",
      "             'arzusu, kÄ±rmÄ±zÄ± zambak desenleri ve bÃ¼yÃ¼k mÃ¼cevher gÃ¶rÃ¼nÃ¼mleri '\n",
      "             'ile koleksiyon tamamlanÄ±yor.\\n'\n",
      "             '\\n'\n",
      "             'Tebrikler Damla!\\n'\n",
      "             '\\n'\n",
      "             '#GencModaTasarimcilari #Koza2023 #KozaYarismasi '\n",
      "             '#TasarimYarismasi #Moda #Fashion #ModaTasarÄ±mÄ±',\n",
      "  'comments_count': 2,\n",
      "  'id': '18144550534306740',\n",
      "  'like_count': 158,\n",
      "  'media_type': 'CAROUSEL_ALBUM',\n",
      "  'media_url': 'https://scontent-sof1-1.cdninstagram.com/v/t51.29350-15/397997154_1016992459537522_4925783512176260397_n.jpg?_nc_cat=106&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=7V_eObkFeK4AX-LMtsK&_nc_ht=scontent-sof1-1.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfDEqDhzaTO3ezV-veT6cJFCOcAEyeVzHR6si9n33N6G5A&oe=6551B6B9',\n",
      "  'timestamp': '2023-11-02 15:49:22',\n",
      "  'username': 'kozayarismasi'},\n",
      " {'caption': 'TÃ¼m TÃ¼rkiye ve Avrupaâ€™ya sevkiyatlarÄ±mÄ±z aralÄ±ksÄ±z devam ediyor! '\n",
      "             'AracÄ±mÄ±z Bursaâ€™dan Orduâ€™ya mÃ¼ÅŸterimizin Ã¼rÃ¼nleri iÃ§in yola '\n",
      "             'Ã§Ä±kÄ±yor.\\n'\n",
      "             '\\n'\n",
      "             'ðŸ‘‰Tuna Mah. Etibank Cad. No:134 Osmangazi/BURSA\\n'\n",
      "             '\\n'\n",
      "             'www.celikbeymobilya.com sitemizden tÃ¼m modelleri '\n",
      "             'inceleyebilirsiniz. \\n'\n",
      "             '\\n'\n",
      "             '#bursa #almanya #fransa',\n",
      "  'comments_count': 0,\n",
      "  'id': '17995331788956693',\n",
      "  'like_count': 99,\n",
      "  'media_type': 'VIDEO',\n",
      "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m82/BF4767CB85BDFB8ADCCCA8F15B8C20B5_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmNsaXBzLnVua25vd24tQzMuNzIwLmRhc2hfYmFzZWxpbmVfMV92MSJ9&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=110&vs=1259525061418244_1441854817&_nc_vs=HBksFQIYT2lnX3hwdl9yZWVsc19wZXJtYW5lbnRfcHJvZC9CRjQ3NjdDQjg1QkRGQjhBRENDQ0E4RjE1QjhDMjBCNV92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dBRWdfaFZfcDVCYk5HZ0NBQTlzVURvZW5mZ3FicV9FQUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJvS3uOiQ0P8%2FFQIoAkMzLBdAJO%2Bdsi0OVhgSZGFzaF9iYXNlbGluZV8xX3YxEQB1AAA%3D&ccb=9-4&oh=00_AfAm22JssMPaUlQe3rpYsFWBhFb5mUgolTCdhV0Xgm4AnA&oe=6556A482&_nc_sid=1d576d&_nc_rid=cd9a998e44',\n",
      "  'timestamp': '2023-08-19 13:46:02',\n",
      "  'username': 'celikbeymobilya'},\n",
      " {'caption': 'ðŸ¤©\\n'\n",
      "             '\\n'\n",
      "             '.\\n'\n",
      "             '.\\n'\n",
      "             'Daha FazlasÄ± Ä°Ã§in BeÄŸenmeyi ve Takip Etmeyi UnutmayÄ±n\\n'\n",
      "             '.\\n'\n",
      "             'girisimci_muhendis âœ…\\n'\n",
      "             '.\\n'\n",
      "             'ðŸ“£Bu Bilgi HakkÄ±nda Ne DÃ¼ÅŸÃ¼nÃ¼yorsunuz.\\n'\n",
      "             '.\\n'\n",
      "             'âœ…GÃ¶rmesini Ä°stediÄŸin ArkadaÅŸÄ±nÄ± Etiketle.\\n'\n",
      "             '.\\n'\n",
      "             'ðŸ””GÃ¶nderi Bildirimlerini AÃ§arak, Bilgileri AnÄ±nda '\n",
      "             'Ã–ÄŸrenebilirsiniz.\\n'\n",
      "             '\\n'\n",
      "             '.\\n'\n",
      "             '\\n'\n",
      "             'Source: Unknown\\n'\n",
      "             'Dm for Credit or removal\\n'\n",
      "             '.\\n'\n",
      "             '.............................................................\\n'\n",
      "             'All rights and credits reserved to the respective owner(s). If '\n",
      "             'you are the main copyright owner rather than the one mentioned '\n",
      "             'here of this content, contact me to claim credit or content '\n",
      "             'removal',\n",
      "  'comments_count': 75,\n",
      "  'id': '18302703232191518',\n",
      "  'like_count': 1224,\n",
      "  'media_type': 'VIDEO',\n",
      "  'media_url': None,\n",
      "  'timestamp': '2023-10-02 06:53:33',\n",
      "  'username': 'girisimci_muhendis'}]\n"
     ]
    }
   ],
   "source": [
    "pprint(output_list[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.model_selection import GridSearchCV\\nfrom sklearn.ensemble import RandomForestRegressor\\n\\n\\n# Define the parameter grid\\nparam_grid = {\\n    \\'n_estimators\\': [50, 100, 200],\\n    \\'max_depth\\': [None, 10, 20, 30],\\n    \\'min_samples_split\\': [2, 5, 10],\\n    \\'min_samples_leaf\\': [1, 2, 4],\\n    \\'max_features\\': [\\'auto\\', \\'sqrt\\', \\'log2\\']\\n}\\n\\n# Initialize the regressor\\nrf = RandomForestRegressor(random_state=42)\\n\\n# Perform grid search\\ngrid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring=\\'neg_mean_squared_error\\', verbose=2, n_jobs=-1)\\ngrid_search.fit(x_train, y_train)\\n\\n# Best parameters and best model\\nprint(\"Best Parameters:\", grid_search.best_params_)\\nbest_model = grid_search.best_estimator_'"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Initialize the regressor\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Best parameters and best model\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from xgboost import XGBRegressor\\nfrom sklearn.neural_network import MLPRegressor\\nfrom sklearn.metrics import mean_squared_error\\nfrom sklearn.model_selection import train_test_split\\nimport numpy as np\\nimport pandas as pd\\n\\n# Prepare the target variable: Average like count per user\\nuser_avg_likes = {}\\nfor username, posts in username2posts_train.items():\\n    avg_likes = np.mean([post.get(\"like_count\", 0) or 0 for post in posts])\\n    user_avg_likes[username] = avg_likes\\n\\n# Create the target array\\ny_like_counts = [user_avg_likes[uname] for uname in train_usernames]\\ny_like_counts = [0 if np.isnan(val) else val for val in y_like_counts]\\n\\n# Train-test split\\nx_train, x_val, y_train, y_val = train_test_split(\\n    x_post_train, y_like_counts, test_size=0.2, random_state=42\\n)\\n\\n# Train an MLP Regressor\\nxgb_regressor = XGBRegressor(\\n    n_estimators=100,   # Number of trees\\n    learning_rate=0.1,  # Step size for updating weights\\n    max_depth=6,        # Depth of each tree\\n    random_state=42\\n)\\n\\n# Train the model\\nxgb_regressor.fit(x_train, y_train)\\n\\ny_val = np.maximum(y_val, 0)\\n\\n\\n\\n# Predict on validation set\\ny_pred_val = xgb_regressor.predict(x_val)\\ny_pred_val = np.maximum(y_pred_val, 0)\\n\\n# Evaluate the model\\nmse = mean_squared_error(y_val, y_pred_val)\\nprint(f\"MSE on validation set: {mse}\")\\n\\n# Predict on test set\\ny_test_pred = xgb_regressor.predict(x_post_test)\\n\\n\\n# Create a DataFrame for test predictions\\ntest_predictions = pd.DataFrame({\\n    \"username\": test_usernames,\\n    \"predicted_like_count\": y_test_pred\\n})\\nprint(test_predictions.head())'"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from xgboost import XGBRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare the target variable: Average like count per user\n",
    "user_avg_likes = {}\n",
    "for username, posts in username2posts_train.items():\n",
    "    avg_likes = np.mean([post.get(\"like_count\", 0) or 0 for post in posts])\n",
    "    user_avg_likes[username] = avg_likes\n",
    "\n",
    "# Create the target array\n",
    "y_like_counts = [user_avg_likes[uname] for uname in train_usernames]\n",
    "y_like_counts = [0 if np.isnan(val) else val for val in y_like_counts]\n",
    "\n",
    "# Train-test split\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_post_train, y_like_counts, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train an MLP Regressor\n",
    "xgb_regressor = XGBRegressor(\n",
    "    n_estimators=100,   # Number of trees\n",
    "    learning_rate=0.1,  # Step size for updating weights\n",
    "    max_depth=6,        # Depth of each tree\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_regressor.fit(x_train, y_train)\n",
    "\n",
    "y_val = np.maximum(y_val, 0)\n",
    "\n",
    "\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred_val = xgb_regressor.predict(x_val)\n",
    "y_pred_val = np.maximum(y_pred_val, 0)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_val, y_pred_val)\n",
    "print(f\"MSE on validation set: {mse}\")\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = xgb_regressor.predict(x_post_test)\n",
    "\n",
    "\n",
    "# Create a DataFrame for test predictions\n",
    "test_predictions = pd.DataFrame({\n",
    "    \"username\": test_usernames,\n",
    "    \"predicted_like_count\": y_test_pred\n",
    "})\n",
    "print(test_predictions.head())'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109845 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 210149\n",
      "[LightGBM] [Info] Number of data points in the train set: 2192, number of used features: 6824\n",
      "[LightGBM] [Info] Start training from score 8.538830\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "MSE on validation set: 3061670607.4357142\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "                     username  predicted_like_count\n",
      "0              beyazyakaliyiz           9599.277991\n",
      "1  totalenergies_istasyonlari             61.663299\n",
      "2                 konforyatak             24.042859\n",
      "3                    ht_kulup            169.114373\n",
      "4                   ajansspor            344.821231\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare the target variable: Average like count per user\n",
    "user_avg_likes = {}\n",
    "for username, posts in username2posts_train.items():\n",
    "    avg_likes = np.mean([post.get(\"like_count\", 0) or 0 for post in posts])\n",
    "    user_avg_likes[username] = avg_likes\n",
    "\n",
    "# Create the target array\n",
    "y_like_counts = [user_avg_likes[uname] for uname in train_usernames]\n",
    "y_like_counts = [0 if np.isnan(val) else val for val in y_like_counts]\n",
    "\n",
    "# Train-test split\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_post_train, y_like_counts, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "lgbm_regressor = LGBMRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.2,\n",
    "    max_depth=30,\n",
    "    min_data_in_leaf=30,\n",
    "    feature_fraction=0.8,\n",
    "    bagging_fraction=0.8,\n",
    "    objective=\"poisson\",  # Poisson objective for count data\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "lgbm_regressor.fit(x_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred_val = lgbm_regressor.predict(x_val)\n",
    "\n",
    "# Ensure non-negative predictions\n",
    "y_pred_val = np.maximum(y_pred_val, 0)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_val, y_pred_val)\n",
    "print(f\"MSE on validation set: {mse}\")\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = lgbm_regressor.predict(x_post_test)\n",
    "\n",
    "# Ensure non-negative predictions\n",
    "y_test_pred = np.maximum(y_test_pred, 0)\n",
    "\n",
    "# Create a DataFrame for test predictions\n",
    "test_predictions = pd.DataFrame({\n",
    "    \"username\": test_usernames,\n",
    "    \"predicted_like_count\": y_test_pred\n",
    "})\n",
    "\n",
    "print(test_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      username  predicted_like_count\n",
      "0               beyazyakaliyiz           9599.277991\n",
      "1   totalenergies_istasyonlari             61.663299\n",
      "2                  konforyatak             24.042859\n",
      "3                     ht_kulup            169.114373\n",
      "4                    ajansspor            344.821231\n",
      "5         yusufelibelediyesi08             73.046840\n",
      "6                     4bros.tr            104.646842\n",
      "7              groundy.kadikoy            455.485480\n",
      "8                drtubagunebak            308.385433\n",
      "9                   nihatcan11             62.915241\n",
      "10               mustafaaakcay            266.759324\n",
      "11                  mtmofamily           9031.409849\n",
      "12                 bonitoperde            133.375867\n",
      "13     balkanlardangelenlezzet             15.089956\n",
      "14      pedagoganne__gulozturk            516.564634\n",
      "15                      iktm34            292.218342\n",
      "16                    yudumyag            246.160252\n",
      "17      bulentozdemir.edebiyat             53.310332\n",
      "18              imtolstoyevski           2257.536026\n",
      "19                 yorkkadikoy            183.092297\n",
      "20                  akkaalinda            124.687309\n",
      "21            mutfaktayusufvar            900.899033\n",
      "22                   antmodern             32.993952\n",
      "23                turkuazkablo             20.811502\n",
      "24             immergasturkiye             17.311790\n",
      "25              vanillaantalya            158.557493\n",
      "26                  ermanyasar           5650.232430\n",
      "27                   tuna.food            213.261363\n",
      "28        rustik.rus.restorani             64.622241\n",
      "29                    ilhansen            624.486530\n",
      "30   institutfrancaisdeturquie            180.440428\n",
      "31      beachandbeyondswimwear             76.845413\n",
      "32             monsternotebook            146.118920\n",
      "33                     bsynctr             34.741067\n",
      "34                     kbbzone             11.446360\n",
      "35                     dbbanyo             52.632162\n",
      "36          istanbulalpplastik            134.464909\n",
      "37        enka_insaat_official             92.952098\n",
      "38              ozyufka.com_tr            141.256374\n",
      "39                nationalturk            400.391671\n",
      "40             sametkaankuyucu           2799.519283\n",
      "41               tutbelediyesi             64.856068\n",
      "42                      odurla            107.533207\n",
      "43                   balagency             29.032744\n",
      "44              sokebelediyesi             93.485780\n",
      "45   dedemanpalandokenskilodge             51.736851\n",
      "46               binefismutfak            385.284090\n",
      "47                tatli.sanati           1286.488522\n",
      "48            evimin.yemekleri            338.208463\n",
      "49                ceyo_turkiye             17.487057\n"
     ]
    }
   ],
   "source": [
    "print(test_predictions.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.isnan(y_val).any() or np.isnan(y_pred_val).any():\n",
    "    print(\"NaN values found in y_val or y_pred_val!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Log Errors: 728.0061700155054\n"
     ]
    }
   ],
   "source": [
    "log_errors = np.log1p(y_val) - np.log1p(y_pred_val)  # Use np.log1p to handle zero values gracefully\n",
    "\n",
    "# Compute the absolute log errors\n",
    "abs_log_errors = np.abs(log_errors)\n",
    "\n",
    "# Sum of log errors\n",
    "sum_log_errors = np.sum(abs_log_errors)\n",
    "\n",
    "print(f\"Sum of Log Errors: {sum_log_errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[401], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m300\u001b[39m],\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.15\u001b[39m],\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_data_in_leaf\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m30\u001b[39m],\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_fraction\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.9\u001b[39m],\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbagging_fraction\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.9\u001b[39m]\n\u001b[0;32m      8\u001b[0m }\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV\n\u001b[0;32m     12\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mlgbm_regressor, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[401], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m300\u001b[39m],\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.15\u001b[39m],\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_data_in_leaf\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m30\u001b[39m],\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_fraction\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.9\u001b[39m],\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbagging_fraction\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.9\u001b[39m]\n\u001b[0;32m      8\u001b[0m }\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV\n\u001b[0;32m     12\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mlgbm_regressor, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m_pydevd_bundle\\\\pydevd_cython.pyx:1698\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle\\\\pydevd_cython.pyx:636\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle\\\\pydevd_cython.pyx:1113\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle\\\\pydevd_cython.pyx:1091\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle\\\\pydevd_cython.pyx:496\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2197\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2194\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2196\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2197\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2199\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2202\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2266\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2263\u001b[0m                 queue\u001b[38;5;241m.\u001b[39mput(internal_cmd)\n\u001b[0;32m   2264\u001b[0m                 wait_timeout \u001b[38;5;241m=\u001b[39m TIMEOUT_FAST\n\u001b[1;32m-> 2266\u001b[0m         \u001b[43mnotify_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2267\u001b[0m         notify_event\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m   2269\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    627\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\threading.py:331\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 331\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    333\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.05, 0.1, 0.15],\n",
    "    'max_depth': [5, 10, -1],\n",
    "    'min_data_in_leaf': [10, 20, 30],\n",
    "    'feature_fraction': [0.8, 0.9],\n",
    "    'bagging_fraction': [0.8, 0.9]\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(estimator=lgbm_regressor, param_grid=param_grid, cv=3, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Osama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation set: 2819676604.812782\n",
      "Sum of Log Errors: 1278.0898952402472\n",
      "                     username  predicted_like_count\n",
      "0              beyazyakaliyiz          10770.258789\n",
      "1  totalenergies_istasyonlari            453.646881\n",
      "2                 konforyatak            370.162567\n",
      "3                    ht_kulup            484.007294\n",
      "4                   ajansspor           2237.495117\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare the target variable: Average like count per user\n",
    "user_avg_likes = {}\n",
    "for username, posts in username2posts_train.items():\n",
    "    avg_likes = np.mean([post.get(\"like_count\", 0) or 0 for post in posts])\n",
    "    user_avg_likes[username] = avg_likes\n",
    "\n",
    "# Create the target array\n",
    "y_like_counts = [user_avg_likes[uname] for uname in train_usernames]\n",
    "y_like_counts = [0 if np.isnan(val) else val for val in y_like_counts]\n",
    "# Train-test split\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_post_train, y_like_counts, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "xgb_regressor = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    objective='reg:squarederror',  # Default; can change to 'reg:poisson' for count data\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_regressor.fit(x_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred_val = xgb_regressor.predict(x_val)\n",
    "\n",
    "# Post-process: Clamp negative predictions to 0\n",
    "y_pred_val = np.maximum(y_pred_val, 0)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_val, y_pred_val)\n",
    "log_errors = np.log1p(y_val) - np.log1p(y_pred_val)  # Use np.log1p to handle zero values\n",
    "abs_log_errors = np.abs(log_errors)\n",
    "sum_log_errors = np.sum(abs_log_errors)\n",
    "\n",
    "print(f\"MSE on validation set: {mse}\")\n",
    "print(f\"Sum of Log Errors: {sum_log_errors}\")\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = xgb_regressor.predict(x_post_test)\n",
    "\n",
    "# Post-process: Clamp negative predictions to 0\n",
    "y_test_pred = np.maximum(y_test_pred, 0)\n",
    "\n",
    "# Create a DataFrame for test predictions\n",
    "test_predictions = pd.DataFrame({\n",
    "    \"username\": test_usernames,\n",
    "    \"predicted_like_count\": y_test_pred\n",
    "})\n",
    "\n",
    "print(test_predictions.head())\n",
    "\n",
    "# Save test predictions to a CSV file\n",
    "#test_predictions.to_csv(\"test_predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Log Errors: 1278.0898952402472\n"
     ]
    }
   ],
   "source": [
    "log_errors = np.log1p(y_val) - np.log1p(y_pred_val)  # Use np.log1p to handle zero values gracefully\n",
    "\n",
    "# Compute the absolute log errors\n",
    "abs_log_errors = np.abs(log_errors)\n",
    "\n",
    "# Sum of log errors\n",
    "sum_log_errors = np.sum(abs_log_errors)\n",
    "\n",
    "print(f\"Sum of Log Errors: {sum_log_errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'user_avg_likes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(avg_likes):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN detected for user: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00musername\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, posts: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mposts\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43muser_avg_likes\u001b[49m[username] \u001b[38;5;241m=\u001b[39m avg_likes\n",
      "\u001b[1;31mNameError\u001b[0m: name 'user_avg_likes' is not defined"
     ]
    }
   ],
   "source": [
    "# Check for NaN in user_avg_likes\n",
    "for username, posts in username2posts_train.items():\n",
    "    avg_likes = np.mean([post.get(\"like_count\", 0) or 0 for post in posts])\n",
    "    if np.isnan(avg_likes):\n",
    "        print(f\"NaN detected for user: {username}, posts: {posts}\")\n",
    "    user_avg_likes[username] = avg_likes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_val_pred_reg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate log errors\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m log_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(y_val_reg \u001b[38;5;241m-\u001b[39m \u001b[43my_val_pred_reg\u001b[49m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Sum of log errors\u001b[39;00m\n\u001b[0;32m      5\u001b[0m sum_log_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(log_errors)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_val_pred_reg' is not defined"
     ]
    }
   ],
   "source": [
    "'''# Calculate log errors\n",
    "log_errors = np.abs(y_val_reg - y_val_pred_reg)\n",
    "\n",
    "# Sum of log errors\n",
    "sum_log_errors = np.sum(log_errors)\n",
    "\n",
    "print(\"Sum of Log Errors:\", sum_log_errors)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
